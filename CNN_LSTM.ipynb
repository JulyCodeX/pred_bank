{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D, Conv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation, LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>291</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>9</td>\n",
       "      <td>may</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>divorced</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>5076</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>7</td>\n",
       "      <td>apr</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>104</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>14</td>\n",
       "      <td>jul</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>-994</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>18</td>\n",
       "      <td>jul</td>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>divorced</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2974</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>21</td>\n",
       "      <td>may</td>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  age         job   marital  education default  balance housing loan  \\\n",
       "0   1   43  management   married   tertiary      no      291     yes   no   \n",
       "1   2   42  technician  divorced    primary      no     5076     yes   no   \n",
       "2   3   47      admin.   married  secondary      no      104     yes  yes   \n",
       "3   4   28  management    single  secondary      no     -994     yes  yes   \n",
       "4   5   42  technician  divorced  secondary      no     2974     yes   no   \n",
       "\n",
       "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
       "0   unknown    9   may       150         2     -1         0  unknown  0  \n",
       "1  cellular    7   apr        99         1    251         2    other  0  \n",
       "2  cellular   14   jul        77         2     -1         0  unknown  0  \n",
       "3  cellular   18   jul       174         2     -1         0  unknown  0  \n",
       "4   unknown   21   may       187         5     -1         0  unknown  0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train = pd.read_csv('train_set.csv', delimiter=',')\n",
    "d_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25318</td>\n",
       "      <td>51</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>174</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>29</td>\n",
       "      <td>jul</td>\n",
       "      <td>308</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25319</td>\n",
       "      <td>32</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>6059</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>20</td>\n",
       "      <td>nov</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25320</td>\n",
       "      <td>60</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>30</td>\n",
       "      <td>jul</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25321</td>\n",
       "      <td>32</td>\n",
       "      <td>student</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>64</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>30</td>\n",
       "      <td>jun</td>\n",
       "      <td>598</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25322</td>\n",
       "      <td>41</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>15</td>\n",
       "      <td>jul</td>\n",
       "      <td>368</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  age         job  marital  education default  balance housing loan  \\\n",
       "0  25318   51   housemaid  married    unknown      no      174      no   no   \n",
       "1  25319   32  management  married   tertiary      no     6059     yes   no   \n",
       "2  25320   60     retired  married    primary      no        0      no   no   \n",
       "3  25321   32     student   single   tertiary      no       64      no   no   \n",
       "4  25322   41   housemaid  married  secondary      no        0     yes  yes   \n",
       "\n",
       "     contact  day month  duration  campaign  pdays  previous poutcome  \n",
       "0  telephone   29   jul       308         3     -1         0  unknown  \n",
       "1   cellular   20   nov       110         2     -1         0  unknown  \n",
       "2  telephone   30   jul       130         3     -1         0  unknown  \n",
       "3   cellular   30   jun       598         4    105         5  failure  \n",
       "4   cellular   15   jul       368         4     -1         0  unknown  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test = pd.read_csv('test_set.csv', delimiter=',')\n",
    "d_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID            int64\n",
       "age           int64\n",
       "job          object\n",
       "marital      object\n",
       "education    object\n",
       "default      object\n",
       "balance       int64\n",
       "housing      object\n",
       "loan         object\n",
       "contact      object\n",
       "day           int64\n",
       "month        object\n",
       "duration      int64\n",
       "campaign      int64\n",
       "pdays         int64\n",
       "previous      int64\n",
       "poutcome     object\n",
       "y             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = d_train\n",
    "dataset_test = d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5076</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2974</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  age  job  marital  education  default  balance  housing  loan  contact  \\\n",
       "0   1   43    4        1          2        0      291        1     0        2   \n",
       "1   2   42    9        0          0        0     5076        1     0        0   \n",
       "2   3   47    0        1          1        0      104        1     1        0   \n",
       "3   4   28    4        2          1        0     -994        1     1        0   \n",
       "4   5   42    9        0          1        0     2974        1     0        2   \n",
       "\n",
       "   day  month  duration  campaign  pdays  previous  poutcome  y  \n",
       "0    9      8       150         2     -1         0         3  0  \n",
       "1    7      0        99         1    251         2         1  0  \n",
       "2   14      5        77         2     -1         0         3  0  \n",
       "3   18      5       174         2     -1         0         3  0  \n",
       "4   21      8       187         5     -1         0         3  0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh = LabelEncoder()\n",
    "for col in dataset_train.columns[dataset_train.dtypes == 'object']:\n",
    "    dataset_train[col] = lh.fit_transform(dataset_train[col])\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "      <td>25317.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12659.000000</td>\n",
       "      <td>40.935379</td>\n",
       "      <td>4.330687</td>\n",
       "      <td>1.167555</td>\n",
       "      <td>1.226291</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>1357.555082</td>\n",
       "      <td>0.553778</td>\n",
       "      <td>0.160327</td>\n",
       "      <td>0.640163</td>\n",
       "      <td>15.835289</td>\n",
       "      <td>5.523166</td>\n",
       "      <td>257.732393</td>\n",
       "      <td>2.772050</td>\n",
       "      <td>40.248766</td>\n",
       "      <td>0.591737</td>\n",
       "      <td>2.558399</td>\n",
       "      <td>0.116957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7308.532719</td>\n",
       "      <td>10.634289</td>\n",
       "      <td>3.269565</td>\n",
       "      <td>0.608091</td>\n",
       "      <td>0.750483</td>\n",
       "      <td>0.131845</td>\n",
       "      <td>2999.822811</td>\n",
       "      <td>0.497109</td>\n",
       "      <td>0.366916</td>\n",
       "      <td>0.897537</td>\n",
       "      <td>8.319480</td>\n",
       "      <td>3.010054</td>\n",
       "      <td>256.975151</td>\n",
       "      <td>3.136097</td>\n",
       "      <td>100.213541</td>\n",
       "      <td>2.568313</td>\n",
       "      <td>0.989615</td>\n",
       "      <td>0.321375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8019.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6330.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12659.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18988.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1435.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25317.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>102127.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3881.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>854.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID           age           job       marital     education  \\\n",
       "count  25317.000000  25317.000000  25317.000000  25317.000000  25317.000000   \n",
       "mean   12659.000000     40.935379      4.330687      1.167555      1.226291   \n",
       "std     7308.532719     10.634289      3.269565      0.608091      0.750483   \n",
       "min        1.000000     18.000000      0.000000      0.000000      0.000000   \n",
       "25%     6330.000000     33.000000      1.000000      1.000000      1.000000   \n",
       "50%    12659.000000     39.000000      4.000000      1.000000      1.000000   \n",
       "75%    18988.000000     48.000000      7.000000      2.000000      2.000000   \n",
       "max    25317.000000     95.000000     11.000000      2.000000      3.000000   \n",
       "\n",
       "            default        balance       housing          loan       contact  \\\n",
       "count  25317.000000   25317.000000  25317.000000  25317.000000  25317.000000   \n",
       "mean       0.017696    1357.555082      0.553778      0.160327      0.640163   \n",
       "std        0.131845    2999.822811      0.497109      0.366916      0.897537   \n",
       "min        0.000000   -8019.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      73.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000     448.000000      1.000000      0.000000      0.000000   \n",
       "75%        0.000000    1435.000000      1.000000      0.000000      2.000000   \n",
       "max        1.000000  102127.000000      1.000000      1.000000      2.000000   \n",
       "\n",
       "                day         month      duration      campaign         pdays  \\\n",
       "count  25317.000000  25317.000000  25317.000000  25317.000000  25317.000000   \n",
       "mean      15.835289      5.523166    257.732393      2.772050     40.248766   \n",
       "std        8.319480      3.010054    256.975151      3.136097    100.213541   \n",
       "min        1.000000      0.000000      0.000000      1.000000     -1.000000   \n",
       "25%        8.000000      3.000000    103.000000      1.000000     -1.000000   \n",
       "50%       16.000000      6.000000    181.000000      2.000000     -1.000000   \n",
       "75%       21.000000      8.000000    317.000000      3.000000     -1.000000   \n",
       "max       31.000000     11.000000   3881.000000     55.000000    854.000000   \n",
       "\n",
       "           previous      poutcome             y  \n",
       "count  25317.000000  25317.000000  25317.000000  \n",
       "mean       0.591737      2.558399      0.116957  \n",
       "std        2.568313      0.989615      0.321375  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.000000      3.000000      0.000000  \n",
       "50%        0.000000      3.000000      0.000000  \n",
       "75%        0.000000      3.000000      0.000000  \n",
       "max      275.000000      3.000000      1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25317, 17), (25317,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_y = dataset_train['y']\n",
    "dataset_train_x = dataset_train[list(set(dataset_train.columns)-set(['y']))]\n",
    "dataset_train_x.shape, dataset_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25318</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>308</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25319</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25320</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25321</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>598</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25322</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>368</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  age  job  marital  education  default  balance  housing  loan  \\\n",
       "0  25318   51    3        1          3        0      174        0     0   \n",
       "1  25319   32    4        1          2        0     6059        1     0   \n",
       "2  25320   60    5        1          0        0        0        0     0   \n",
       "3  25321   32    8        2          2        0       64        0     0   \n",
       "4  25322   41    3        1          1        0        0        1     1   \n",
       "\n",
       "   contact  day  month  duration  campaign  pdays  previous  poutcome  \n",
       "0        1   29      5       308         3     -1         0         3  \n",
       "1        0   20      9       110         2     -1         0         3  \n",
       "2        1   30      5       130         3     -1         0         3  \n",
       "3        0   30      6       598         4    105         5         0  \n",
       "4        0   15      5       368         4     -1         0         3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in dataset_test.columns[dataset_test.dtypes == 'object']:\n",
    "    dataset_test[col] = lh.fit_transform(dataset_test[col])\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "      <td>10852.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30743.500000</td>\n",
       "      <td>41.040638</td>\n",
       "      <td>4.372650</td>\n",
       "      <td>1.164947</td>\n",
       "      <td>1.226502</td>\n",
       "      <td>0.018153</td>\n",
       "      <td>1393.157298</td>\n",
       "      <td>0.547272</td>\n",
       "      <td>0.160984</td>\n",
       "      <td>0.637486</td>\n",
       "      <td>15.680151</td>\n",
       "      <td>5.509399</td>\n",
       "      <td>257.206137</td>\n",
       "      <td>2.770365</td>\n",
       "      <td>39.954755</td>\n",
       "      <td>0.546443</td>\n",
       "      <td>2.558975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3132.846895</td>\n",
       "      <td>10.652369</td>\n",
       "      <td>3.265972</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.748990</td>\n",
       "      <td>0.133512</td>\n",
       "      <td>3318.497101</td>\n",
       "      <td>0.497783</td>\n",
       "      <td>0.367533</td>\n",
       "      <td>0.897054</td>\n",
       "      <td>8.302317</td>\n",
       "      <td>3.009708</td>\n",
       "      <td>250.480906</td>\n",
       "      <td>3.063481</td>\n",
       "      <td>99.524056</td>\n",
       "      <td>1.805938</td>\n",
       "      <td>0.993169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25318.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2604.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28030.750000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30743.500000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33456.250000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1440.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36169.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81204.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3102.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID           age           job       marital     education  \\\n",
       "count  10852.000000  10852.000000  10852.000000  10852.000000  10852.000000   \n",
       "mean   30743.500000     41.040638      4.372650      1.164947      1.226502   \n",
       "std     3132.846895     10.652369      3.265972      0.602941      0.748990   \n",
       "min    25318.000000     18.000000      0.000000      0.000000      0.000000   \n",
       "25%    28030.750000     33.000000      1.000000      1.000000      1.000000   \n",
       "50%    30743.500000     39.000000      4.000000      1.000000      1.000000   \n",
       "75%    33456.250000     49.000000      7.000000      2.000000      2.000000   \n",
       "max    36169.000000     94.000000     11.000000      2.000000      3.000000   \n",
       "\n",
       "            default       balance       housing          loan       contact  \\\n",
       "count  10852.000000  10852.000000  10852.000000  10852.000000  10852.000000   \n",
       "mean       0.018153   1393.157298      0.547272      0.160984      0.637486   \n",
       "std        0.133512   3318.497101      0.497783      0.367533      0.897054   \n",
       "min        0.000000  -2604.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000     72.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000    450.000000      1.000000      0.000000      0.000000   \n",
       "75%        0.000000   1440.250000      1.000000      0.000000      2.000000   \n",
       "max        1.000000  81204.000000      1.000000      1.000000      2.000000   \n",
       "\n",
       "                day         month      duration      campaign         pdays  \\\n",
       "count  10852.000000  10852.000000  10852.000000  10852.000000  10852.000000   \n",
       "mean      15.680151      5.509399    257.206137      2.770365     39.954755   \n",
       "std        8.302317      3.009708    250.480906      3.063481     99.524056   \n",
       "min        1.000000      0.000000      0.000000      1.000000     -1.000000   \n",
       "25%        8.000000      3.000000    102.000000      1.000000     -1.000000   \n",
       "50%       16.000000      6.000000    181.000000      2.000000     -1.000000   \n",
       "75%       21.000000      8.000000    322.000000      3.000000     -1.000000   \n",
       "max       31.000000     11.000000   3102.000000     58.000000    871.000000   \n",
       "\n",
       "           previous      poutcome  \n",
       "count  10852.000000  10852.000000  \n",
       "mean       0.546443      2.558975  \n",
       "std        1.805938      0.993169  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      3.000000  \n",
       "50%        0.000000      3.000000  \n",
       "75%        0.000000      3.000000  \n",
       "max       51.000000      3.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>campaign</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>default</th>\n",
       "      <th>duration</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>job</th>\n",
       "      <th>loan</th>\n",
       "      <th>marital</th>\n",
       "      <th>month</th>\n",
       "      <th>pdays</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>291</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>5076</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>-994</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>2974</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  age  balance  campaign  contact  day  default  duration  education  \\\n",
       "0   1   43      291         2        2    9        0       150          2   \n",
       "1   2   42     5076         1        0    7        0        99          0   \n",
       "2   3   47      104         2        0   14        0        77          1   \n",
       "3   4   28     -994         2        0   18        0       174          1   \n",
       "4   5   42     2974         5        2   21        0       187          1   \n",
       "\n",
       "   housing  job  loan  marital  month  pdays  poutcome  previous  \n",
       "0        1    4     0        1      8     -1         3         0  \n",
       "1        1    9     0        0      0    251         1         2  \n",
       "2        1    0     1        1      5     -1         3         0  \n",
       "3        1    4     1        2      5     -1         3         0  \n",
       "4        1    9     0        0      8     -1         3         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_all = pd.concat((dataset_train_x, dataset_test))\n",
    "dataset_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int32, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "E:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int32, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>campaign</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>default</th>\n",
       "      <th>duration</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>job</th>\n",
       "      <th>loan</th>\n",
       "      <th>marital</th>\n",
       "      <th>month</th>\n",
       "      <th>pdays</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.347627</td>\n",
       "      <td>-0.247734</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.421799</td>\n",
       "      <td>1.031508</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.105027</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.411587</td>\n",
       "      <td>0.445592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1.196503</td>\n",
       "      <td>-0.568823</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.621770</td>\n",
       "      <td>-1.635108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.424748</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.108300</td>\n",
       "      <td>-1.573274</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.407972</td>\n",
       "      <td>-0.247734</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.708032</td>\n",
       "      <td>-0.301800</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.328848</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.411587</td>\n",
       "      <td>0.445592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.762299</td>\n",
       "      <td>-0.247734</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.327695</td>\n",
       "      <td>-0.301800</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.105027</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.411587</td>\n",
       "      <td>0.445592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>0.518183</td>\n",
       "      <td>0.715532</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.276722</td>\n",
       "      <td>-0.301800</td>\n",
       "      <td>1</td>\n",
       "      <td>1.424748</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.411587</td>\n",
       "      <td>0.445592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  age   balance  campaign  contact  day  default  duration  education  \\\n",
       "0   1   43 -0.347627 -0.247734        2    9        0 -0.421799   1.031508   \n",
       "1   2   42  1.196503 -0.568823        0    7        0 -0.621770  -1.635108   \n",
       "2   3   47 -0.407972 -0.247734        0   14        0 -0.708032  -0.301800   \n",
       "3   4   28 -0.762299 -0.247734        0   18        0 -0.327695  -0.301800   \n",
       "4   5   42  0.518183  0.715532        2   21        0 -0.276722  -0.301800   \n",
       "\n",
       "   housing       job  loan  marital  month     pdays  poutcome  previous  \n",
       "0        1 -0.105027     0        1      8 -0.411587  0.445592         0  \n",
       "1        1  1.424748     0        0      0  2.108300 -1.573274         2  \n",
       "2        1 -1.328848     1        1      5 -0.411587  0.445592         0  \n",
       "3        1 -0.105027     1        2      5 -0.411587  0.445592         0  \n",
       "4        1  1.424748     0        0      8 -0.411587  0.445592         0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "dataset_all[['balance', 'campaign', 'duration', 'education', 'pdays', 'poutcome', 'job']] = ss.fit_transform(dataset_all[['balance', 'campaign', 'duration', 'education', 'pdays', 'poutcome', 'job']])\n",
    "dataset_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset_all.iloc[:25317,1:]\n",
    "test_set = dataset_all.iloc[25317:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25317, 17)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_all = pd.concat([train_set, dataset_train_y], axis=1)\n",
    "train_set_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_counter = Counter(train_set['age'].values, reverse=False)\n",
    "#age_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_set.values, dataset_train_y.values, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20253, 16), (5064, 16))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_fcn(units_list=[16], optimizer='adam', init='normal'):\n",
    "    model = Sequential()\n",
    "    units = units_list[0]\n",
    "    model.add(Dense(units=units, activation='relu', input_dim=16, kernel_initializer=init))\n",
    "    for units in units_list[1:]:\n",
    "        model.add(Dense(units=units, activation='relu', kernel_initializer=init))\n",
    "    model.add(Dense(units=1, activation='sigmoid', kernel_initializer=init))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17359/17359 [==============================] - 2s 102us/step - loss: 0.3133 - acc: 0.8877\n",
      "Epoch 2/100\n",
      "17359/17359 [==============================] - 1s 63us/step - loss: 0.2615 - acc: 0.8926\n",
      "Epoch 3/100\n",
      "17359/17359 [==============================] - 1s 64us/step - loss: 0.2555 - acc: 0.8933\n",
      "Epoch 4/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2513 - acc: 0.8948\n",
      "Epoch 5/100\n",
      "17359/17359 [==============================] - ETA: 0s - loss: 0.2481 - acc: 0.894 - 1s 66us/step - loss: 0.2478 - acc: 0.8952\n",
      "Epoch 6/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2446 - acc: 0.8975\n",
      "Epoch 7/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2438 - acc: 0.8987\n",
      "Epoch 8/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2419 - acc: 0.8992\n",
      "Epoch 9/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2410 - acc: 0.8999\n",
      "Epoch 10/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2401 - acc: 0.9002\n",
      "Epoch 11/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2393 - acc: 0.9009\n",
      "Epoch 12/100\n",
      "17359/17359 [==============================] - 1s 69us/step - loss: 0.2388 - acc: 0.9014\n",
      "Epoch 13/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2388 - acc: 0.8996\n",
      "Epoch 14/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2378 - acc: 0.9020\n",
      "Epoch 15/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2381 - acc: 0.9018\n",
      "Epoch 16/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2377 - acc: 0.9028\n",
      "Epoch 17/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2374 - acc: 0.9017\n",
      "Epoch 18/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2375 - acc: 0.9011\n",
      "Epoch 19/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2361 - acc: 0.9035\n",
      "Epoch 20/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2368 - acc: 0.9006\n",
      "Epoch 21/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2361 - acc: 0.9025\n",
      "Epoch 22/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2354 - acc: 0.9029\n",
      "Epoch 23/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2362 - acc: 0.9015\n",
      "Epoch 24/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2352 - acc: 0.9020\n",
      "Epoch 25/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2350 - acc: 0.9010\n",
      "Epoch 26/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2343 - acc: 0.9017\n",
      "Epoch 27/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2341 - acc: 0.9032\n",
      "Epoch 28/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2335 - acc: 0.9014\n",
      "Epoch 29/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2343 - acc: 0.9029\n",
      "Epoch 30/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2324 - acc: 0.9026\n",
      "Epoch 31/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2321 - acc: 0.9041\n",
      "Epoch 32/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2319 - acc: 0.9033\n",
      "Epoch 33/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2322 - acc: 0.9018\n",
      "Epoch 34/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2307 - acc: 0.9022\n",
      "Epoch 35/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2296 - acc: 0.9040\n",
      "Epoch 36/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2303 - acc: 0.9025\n",
      "Epoch 37/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2299 - acc: 0.9041\n",
      "Epoch 38/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2278 - acc: 0.9042\n",
      "Epoch 39/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2275 - acc: 0.9039\n",
      "Epoch 40/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2284 - acc: 0.9050\n",
      "Epoch 41/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2277 - acc: 0.9049\n",
      "Epoch 42/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2275 - acc: 0.9052\n",
      "Epoch 43/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2272 - acc: 0.9045\n",
      "Epoch 44/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2268 - acc: 0.9033\n",
      "Epoch 45/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2269 - acc: 0.9057\n",
      "Epoch 46/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2265 - acc: 0.9048\n",
      "Epoch 47/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2265 - acc: 0.9053\n",
      "Epoch 48/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2263 - acc: 0.9058\n",
      "Epoch 49/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2259 - acc: 0.9055\n",
      "Epoch 50/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2262 - acc: 0.9056\n",
      "Epoch 51/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2247 - acc: 0.9058\n",
      "Epoch 52/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2242 - acc: 0.9055\n",
      "Epoch 53/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2250 - acc: 0.9053\n",
      "Epoch 54/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2250 - acc: 0.9047\n",
      "Epoch 55/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2242 - acc: 0.9057\n",
      "Epoch 56/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2235 - acc: 0.9058\n",
      "Epoch 57/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2242 - acc: 0.9054\n",
      "Epoch 58/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2236 - acc: 0.9055\n",
      "Epoch 59/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2229 - acc: 0.9059\n",
      "Epoch 60/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2234 - acc: 0.9074\n",
      "Epoch 61/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2225 - acc: 0.9057\n",
      "Epoch 62/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2224 - acc: 0.9058\n",
      "Epoch 63/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2233 - acc: 0.9056\n",
      "Epoch 64/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2229 - acc: 0.9063\n",
      "Epoch 65/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2231 - acc: 0.9058\n",
      "Epoch 66/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2224 - acc: 0.9055\n",
      "Epoch 67/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2223 - acc: 0.9068\n",
      "Epoch 68/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2221 - acc: 0.9061\n",
      "Epoch 69/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2225 - acc: 0.9071\n",
      "Epoch 70/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2225 - acc: 0.9076\n",
      "Epoch 71/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2210 - acc: 0.9049\n",
      "Epoch 72/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2224 - acc: 0.9060\n",
      "Epoch 73/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2217 - acc: 0.9062\n",
      "Epoch 74/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2223 - acc: 0.9063\n",
      "Epoch 75/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2217 - acc: 0.9067\n",
      "Epoch 76/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2222 - acc: 0.9067\n",
      "Epoch 77/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2213 - acc: 0.9066\n",
      "Epoch 78/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2212 - acc: 0.9066\n",
      "Epoch 79/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2213 - acc: 0.9064\n",
      "Epoch 80/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2210 - acc: 0.9053\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2211 - acc: 0.9068\n",
      "Epoch 82/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2203 - acc: 0.9073\n",
      "Epoch 83/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2206 - acc: 0.9069\n",
      "Epoch 84/100\n",
      "17359/17359 [==============================] - 1s 64us/step - loss: 0.2211 - acc: 0.9059\n",
      "Epoch 85/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2208 - acc: 0.9058\n",
      "Epoch 86/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2215 - acc: 0.9065\n",
      "Epoch 87/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2203 - acc: 0.9070\n",
      "Epoch 88/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2200 - acc: 0.9062\n",
      "Epoch 89/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2205 - acc: 0.9067\n",
      "Epoch 90/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2201 - acc: 0.9062: 1s - los\n",
      "Epoch 91/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2206 - acc: 0.9071: 0s - loss: 0.2206 - acc:\n",
      "Epoch 92/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2207 - acc: 0.9060\n",
      "Epoch 93/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2201 - acc: 0.9073\n",
      "Epoch 94/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2201 - acc: 0.9071\n",
      "Epoch 95/100\n",
      "17359/17359 [==============================] - 1s 64us/step - loss: 0.2196 - acc: 0.9077\n",
      "Epoch 96/100\n",
      "17359/17359 [==============================] - 1s 64us/step - loss: 0.2201 - acc: 0.9060\n",
      "Epoch 97/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2196 - acc: 0.9072\n",
      "Epoch 98/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2200 - acc: 0.9063\n",
      "Epoch 99/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2189 - acc: 0.9060\n",
      "Epoch 100/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2202 - acc: 0.9070\n",
      "2894/2894 [==============================] - 0s 101us/step\n",
      "Epoch 1/100\n",
      "17359/17359 [==============================] - 2s 103us/step - loss: 0.3135 - acc: 0.8871\n",
      "Epoch 2/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2573 - acc: 0.8935\n",
      "Epoch 3/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2511 - acc: 0.8937\n",
      "Epoch 4/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2480 - acc: 0.8957\n",
      "Epoch 5/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2452 - acc: 0.8979\n",
      "Epoch 6/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2433 - acc: 0.8975\n",
      "Epoch 7/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2416 - acc: 0.8988\n",
      "Epoch 8/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2408 - acc: 0.8986\n",
      "Epoch 9/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2397 - acc: 0.8996\n",
      "Epoch 10/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2396 - acc: 0.8997\n",
      "Epoch 11/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2392 - acc: 0.8988\n",
      "Epoch 12/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2388 - acc: 0.8988\n",
      "Epoch 13/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2376 - acc: 0.8999\n",
      "Epoch 14/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2379 - acc: 0.9002\n",
      "Epoch 15/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2373 - acc: 0.9004\n",
      "Epoch 16/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2376 - acc: 0.9007\n",
      "Epoch 17/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2376 - acc: 0.9005\n",
      "Epoch 18/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2367 - acc: 0.9007\n",
      "Epoch 19/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2360 - acc: 0.9029\n",
      "Epoch 20/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2360 - acc: 0.9003\n",
      "Epoch 21/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2358 - acc: 0.9012\n",
      "Epoch 22/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2360 - acc: 0.8999\n",
      "Epoch 23/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2355 - acc: 0.9007\n",
      "Epoch 24/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2351 - acc: 0.9013\n",
      "Epoch 25/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2349 - acc: 0.9018\n",
      "Epoch 26/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2347 - acc: 0.9020\n",
      "Epoch 27/100\n",
      "17359/17359 [==============================] - 1s 64us/step - loss: 0.2339 - acc: 0.9016\n",
      "Epoch 28/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2336 - acc: 0.9003\n",
      "Epoch 29/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2337 - acc: 0.9011\n",
      "Epoch 30/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2335 - acc: 0.9012\n",
      "Epoch 31/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2332 - acc: 0.9010\n",
      "Epoch 32/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2330 - acc: 0.9021\n",
      "Epoch 33/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2329 - acc: 0.9026\n",
      "Epoch 34/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2328 - acc: 0.9006\n",
      "Epoch 35/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2326 - acc: 0.9013\n",
      "Epoch 36/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2326 - acc: 0.9014\n",
      "Epoch 37/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2327 - acc: 0.9017\n",
      "Epoch 38/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2318 - acc: 0.9028\n",
      "Epoch 39/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2319 - acc: 0.9033\n",
      "Epoch 40/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2322 - acc: 0.9024\n",
      "Epoch 41/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2316 - acc: 0.9018\n",
      "Epoch 42/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2321 - acc: 0.9019\n",
      "Epoch 43/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2322 - acc: 0.9033\n",
      "Epoch 44/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2321 - acc: 0.9023\n",
      "Epoch 45/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2315 - acc: 0.9015\n",
      "Epoch 46/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2316 - acc: 0.9025\n",
      "Epoch 47/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2320 - acc: 0.9010\n",
      "Epoch 48/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2316 - acc: 0.9008\n",
      "Epoch 49/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2307 - acc: 0.9017\n",
      "Epoch 50/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2312 - acc: 0.9013\n",
      "Epoch 51/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2314 - acc: 0.9011\n",
      "Epoch 52/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2311 - acc: 0.9022\n",
      "Epoch 53/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2316 - acc: 0.9022\n",
      "Epoch 54/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2310 - acc: 0.9018\n",
      "Epoch 55/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2312 - acc: 0.9021\n",
      "Epoch 56/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2311 - acc: 0.9033\n",
      "Epoch 57/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2307 - acc: 0.9022\n",
      "Epoch 58/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2304 - acc: 0.9020\n",
      "Epoch 59/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2306 - acc: 0.9023\n",
      "Epoch 60/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2309 - acc: 0.9023\n",
      "Epoch 61/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2308 - acc: 0.9021\n",
      "Epoch 62/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2305 - acc: 0.9024\n",
      "Epoch 63/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2297 - acc: 0.9028\n",
      "Epoch 64/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2303 - acc: 0.9022\n",
      "Epoch 65/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2305 - acc: 0.9006\n",
      "Epoch 66/100\n",
      "17359/17359 [==============================] - 1s 64us/step - loss: 0.2301 - acc: 0.9024\n",
      "Epoch 67/100\n",
      "17359/17359 [==============================] - 1s 63us/step - loss: 0.2299 - acc: 0.9021\n",
      "Epoch 68/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2301 - acc: 0.9024\n",
      "Epoch 69/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2295 - acc: 0.9034\n",
      "Epoch 70/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2296 - acc: 0.9025\n",
      "Epoch 71/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2294 - acc: 0.9024\n",
      "Epoch 72/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2298 - acc: 0.9030\n",
      "Epoch 73/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2296 - acc: 0.9019\n",
      "Epoch 74/100\n",
      "17359/17359 [==============================] - 1s 74us/step - loss: 0.2288 - acc: 0.9031\n",
      "Epoch 75/100\n",
      "17359/17359 [==============================] - 1s 68us/step - loss: 0.2290 - acc: 0.9022\n",
      "Epoch 76/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2293 - acc: 0.9017\n",
      "Epoch 77/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2296 - acc: 0.9023\n",
      "Epoch 78/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2293 - acc: 0.9024\n",
      "Epoch 79/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2283 - acc: 0.9016\n",
      "Epoch 80/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2283 - acc: 0.9036\n",
      "Epoch 81/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2288 - acc: 0.9017\n",
      "Epoch 82/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2287 - acc: 0.9020\n",
      "Epoch 83/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2287 - acc: 0.9028\n",
      "Epoch 84/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2291 - acc: 0.9019\n",
      "Epoch 85/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2285 - acc: 0.9029\n",
      "Epoch 86/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2287 - acc: 0.9032\n",
      "Epoch 87/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2286 - acc: 0.9033\n",
      "Epoch 88/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2285 - acc: 0.9022\n",
      "Epoch 89/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2286 - acc: 0.9030\n",
      "Epoch 90/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2283 - acc: 0.9025\n",
      "Epoch 91/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2275 - acc: 0.9039\n",
      "Epoch 92/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2276 - acc: 0.9028\n",
      "Epoch 93/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2285 - acc: 0.9025\n",
      "Epoch 94/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2283 - acc: 0.9030\n",
      "Epoch 95/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2279 - acc: 0.9028\n",
      "Epoch 96/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2281 - acc: 0.9035\n",
      "Epoch 97/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2277 - acc: 0.9032\n",
      "Epoch 98/100\n",
      "17359/17359 [==============================] - 1s 67us/step - loss: 0.2281 - acc: 0.9025\n",
      "Epoch 99/100\n",
      "17359/17359 [==============================] - 1s 66us/step - loss: 0.2278 - acc: 0.9021\n",
      "Epoch 100/100\n",
      "17359/17359 [==============================] - 1s 65us/step - loss: 0.2282 - acc: 0.9015\n",
      "2894/2894 [==============================] - 0s 100us/step\n",
      "Epoch 1/100\n",
      "17360/17360 [==============================] - 2s 120us/step - loss: 0.3251 - acc: 0.8840\n",
      "Epoch 2/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2629 - acc: 0.8905\n",
      "Epoch 3/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2551 - acc: 0.8937\n",
      "Epoch 4/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2506 - acc: 0.8956\n",
      "Epoch 5/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2485 - acc: 0.8967\n",
      "Epoch 6/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2451 - acc: 0.8982\n",
      "Epoch 7/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2442 - acc: 0.8988\n",
      "Epoch 8/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2433 - acc: 0.8995\n",
      "Epoch 9/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2423 - acc: 0.8992\n",
      "Epoch 10/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2417 - acc: 0.8991\n",
      "Epoch 11/100\n",
      "17360/17360 [==============================] - 1s 65us/step - loss: 0.2408 - acc: 0.9007\n",
      "Epoch 12/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2400 - acc: 0.8994\n",
      "Epoch 13/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2400 - acc: 0.9020\n",
      "Epoch 14/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2395 - acc: 0.9014\n",
      "Epoch 15/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2394 - acc: 0.9019\n",
      "Epoch 16/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2403 - acc: 0.9002\n",
      "Epoch 17/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2391 - acc: 0.9016\n",
      "Epoch 18/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2394 - acc: 0.9014\n",
      "Epoch 19/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2388 - acc: 0.9026\n",
      "Epoch 20/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2384 - acc: 0.9011\n",
      "Epoch 21/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2390 - acc: 0.9017\n",
      "Epoch 22/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2385 - acc: 0.9005\n",
      "Epoch 23/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2377 - acc: 0.9009\n",
      "Epoch 24/100\n",
      "17360/17360 [==============================] - 1s 65us/step - loss: 0.2377 - acc: 0.9015\n",
      "Epoch 25/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2376 - acc: 0.9009\n",
      "Epoch 26/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2370 - acc: 0.9021\n",
      "Epoch 27/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2374 - acc: 0.9008\n",
      "Epoch 28/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2370 - acc: 0.9012\n",
      "Epoch 29/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2367 - acc: 0.9024\n",
      "Epoch 30/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2365 - acc: 0.9021\n",
      "Epoch 31/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2363 - acc: 0.9024\n",
      "Epoch 32/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2362 - acc: 0.9007\n",
      "Epoch 33/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2356 - acc: 0.9022\n",
      "Epoch 34/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2357 - acc: 0.9020\n",
      "Epoch 35/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2359 - acc: 0.9021\n",
      "Epoch 36/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2352 - acc: 0.9031: 0s - loss: 0.2378 -\n",
      "Epoch 37/100\n",
      "17360/17360 [==============================] - 1s 77us/step - loss: 0.2353 - acc: 0.9026\n",
      "Epoch 38/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2343 - acc: 0.9032\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2350 - acc: 0.9031\n",
      "Epoch 40/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2353 - acc: 0.9014\n",
      "Epoch 41/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2344 - acc: 0.9026\n",
      "Epoch 42/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2345 - acc: 0.9025\n",
      "Epoch 43/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2337 - acc: 0.9028\n",
      "Epoch 44/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2339 - acc: 0.9031\n",
      "Epoch 45/100\n",
      "17360/17360 [==============================] - 1s 81us/step - loss: 0.2327 - acc: 0.9021\n",
      "Epoch 46/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2335 - acc: 0.9041\n",
      "Epoch 47/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2332 - acc: 0.9010\n",
      "Epoch 48/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2326 - acc: 0.9036\n",
      "Epoch 49/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2332 - acc: 0.9030\n",
      "Epoch 50/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2334 - acc: 0.9032\n",
      "Epoch 51/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2327 - acc: 0.9044\n",
      "Epoch 52/100\n",
      "17360/17360 [==============================] - 1s 76us/step - loss: 0.2329 - acc: 0.9031\n",
      "Epoch 53/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2324 - acc: 0.9036\n",
      "Epoch 54/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2328 - acc: 0.9028\n",
      "Epoch 55/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2320 - acc: 0.9049\n",
      "Epoch 56/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2321 - acc: 0.9023\n",
      "Epoch 57/100\n",
      "17360/17360 [==============================] - 1s 78us/step - loss: 0.2319 - acc: 0.9041\n",
      "Epoch 58/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2307 - acc: 0.9035\n",
      "Epoch 59/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2324 - acc: 0.9033\n",
      "Epoch 60/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2311 - acc: 0.9034\n",
      "Epoch 61/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2310 - acc: 0.9030\n",
      "Epoch 62/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2306 - acc: 0.9033\n",
      "Epoch 63/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2315 - acc: 0.9043\n",
      "Epoch 64/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2310 - acc: 0.9037\n",
      "Epoch 65/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2303 - acc: 0.9048\n",
      "Epoch 66/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2307 - acc: 0.9047\n",
      "Epoch 67/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2307 - acc: 0.9041\n",
      "Epoch 68/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2307 - acc: 0.9035\n",
      "Epoch 69/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2304 - acc: 0.9039\n",
      "Epoch 70/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2303 - acc: 0.9041\n",
      "Epoch 71/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2304 - acc: 0.9033\n",
      "Epoch 72/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2309 - acc: 0.9034\n",
      "Epoch 73/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2298 - acc: 0.9052\n",
      "Epoch 74/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2303 - acc: 0.9047\n",
      "Epoch 75/100\n",
      "17360/17360 [==============================] - 1s 65us/step - loss: 0.2302 - acc: 0.9045\n",
      "Epoch 76/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2299 - acc: 0.9041\n",
      "Epoch 77/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2301 - acc: 0.9035\n",
      "Epoch 78/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2301 - acc: 0.9033\n",
      "Epoch 79/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2299 - acc: 0.9056\n",
      "Epoch 80/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2299 - acc: 0.9044\n",
      "Epoch 81/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2305 - acc: 0.9039\n",
      "Epoch 82/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2294 - acc: 0.9040\n",
      "Epoch 83/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2293 - acc: 0.9043\n",
      "Epoch 84/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2301 - acc: 0.9041\n",
      "Epoch 85/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2296 - acc: 0.9052\n",
      "Epoch 86/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2300 - acc: 0.9047\n",
      "Epoch 87/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2296 - acc: 0.9045\n",
      "Epoch 88/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2295 - acc: 0.9049\n",
      "Epoch 89/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2295 - acc: 0.9047\n",
      "Epoch 90/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2301 - acc: 0.9043\n",
      "Epoch 91/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2291 - acc: 0.9039\n",
      "Epoch 92/100\n",
      "17360/17360 [==============================] - 1s 64us/step - loss: 0.2289 - acc: 0.9040\n",
      "Epoch 93/100\n",
      "17360/17360 [==============================] - 1s 65us/step - loss: 0.2292 - acc: 0.9038\n",
      "Epoch 94/100\n",
      "17360/17360 [==============================] - 1s 65us/step - loss: 0.2293 - acc: 0.9046\n",
      "Epoch 95/100\n",
      "17360/17360 [==============================] - 1s 65us/step - loss: 0.2298 - acc: 0.9045\n",
      "Epoch 96/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2288 - acc: 0.9040\n",
      "Epoch 97/100\n",
      "17360/17360 [==============================] - 1s 65us/step - loss: 0.2289 - acc: 0.9042\n",
      "Epoch 98/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2287 - acc: 0.9033\n",
      "Epoch 99/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2289 - acc: 0.9040\n",
      "Epoch 100/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2287 - acc: 0.9041\n",
      "2893/2893 [==============================] - 0s 110us/step\n",
      "Epoch 1/100\n",
      "17360/17360 [==============================] - 2s 109us/step - loss: 0.3401 - acc: 0.8760\n",
      "Epoch 2/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2669 - acc: 0.8900\n",
      "Epoch 3/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2562 - acc: 0.8927\n",
      "Epoch 4/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2527 - acc: 0.8920\n",
      "Epoch 5/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2490 - acc: 0.8959\n",
      "Epoch 6/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2456 - acc: 0.8960\n",
      "Epoch 7/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2440 - acc: 0.8971\n",
      "Epoch 8/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2429 - acc: 0.8987\n",
      "Epoch 9/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2410 - acc: 0.8988\n",
      "Epoch 10/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2400 - acc: 0.8988\n",
      "Epoch 11/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2391 - acc: 0.8987\n",
      "Epoch 12/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2382 - acc: 0.9001\n",
      "Epoch 13/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2380 - acc: 0.8988\n",
      "Epoch 14/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2371 - acc: 0.9008\n",
      "Epoch 15/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2360 - acc: 0.9006\n",
      "Epoch 16/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2358 - acc: 0.8997\n",
      "Epoch 17/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2357 - acc: 0.9005\n",
      "Epoch 18/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2343 - acc: 0.9013\n",
      "Epoch 19/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2347 - acc: 0.9011\n",
      "Epoch 20/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2345 - acc: 0.9005\n",
      "Epoch 21/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2338 - acc: 0.9010\n",
      "Epoch 22/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2338 - acc: 0.9020: 1s - loss:\n",
      "Epoch 23/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2334 - acc: 0.9025\n",
      "Epoch 24/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2333 - acc: 0.9016\n",
      "Epoch 25/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2325 - acc: 0.9022\n",
      "Epoch 26/100\n",
      "17360/17360 [==============================] - 1s 85us/step - loss: 0.2334 - acc: 0.9020\n",
      "Epoch 27/100\n",
      "17360/17360 [==============================] - 2s 99us/step - loss: 0.2333 - acc: 0.9032\n",
      "Epoch 28/100\n",
      "17360/17360 [==============================] - 2s 88us/step - loss: 0.2323 - acc: 0.9026\n",
      "Epoch 29/100\n",
      "17360/17360 [==============================] - 2s 99us/step - loss: 0.2322 - acc: 0.9020\n",
      "Epoch 30/100\n",
      "17360/17360 [==============================] - 2s 96us/step - loss: 0.2319 - acc: 0.9022\n",
      "Epoch 31/100\n",
      "17360/17360 [==============================] - 2s 96us/step - loss: 0.2319 - acc: 0.9030\n",
      "Epoch 32/100\n",
      "17360/17360 [==============================] - 2s 102us/step - loss: 0.2312 - acc: 0.9037\n",
      "Epoch 33/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2313 - acc: 0.9034\n",
      "Epoch 34/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2317 - acc: 0.9025\n",
      "Epoch 35/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2319 - acc: 0.9022\n",
      "Epoch 36/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2309 - acc: 0.9022\n",
      "Epoch 37/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2313 - acc: 0.9028\n",
      "Epoch 38/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2309 - acc: 0.9023\n",
      "Epoch 39/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2301 - acc: 0.9031\n",
      "Epoch 40/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2310 - acc: 0.9036\n",
      "Epoch 41/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2302 - acc: 0.9032\n",
      "Epoch 42/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2300 - acc: 0.9037\n",
      "Epoch 43/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2300 - acc: 0.9029\n",
      "Epoch 44/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2303 - acc: 0.9037\n",
      "Epoch 45/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2307 - acc: 0.9022\n",
      "Epoch 46/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2289 - acc: 0.9042\n",
      "Epoch 47/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2290 - acc: 0.9039\n",
      "Epoch 48/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2294 - acc: 0.9045\n",
      "Epoch 49/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2291 - acc: 0.9036\n",
      "Epoch 50/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2289 - acc: 0.9036\n",
      "Epoch 51/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2290 - acc: 0.9039\n",
      "Epoch 52/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2291 - acc: 0.9029\n",
      "Epoch 53/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2289 - acc: 0.9040\n",
      "Epoch 54/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2286 - acc: 0.9033\n",
      "Epoch 55/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2283 - acc: 0.9033\n",
      "Epoch 56/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2281 - acc: 0.9055\n",
      "Epoch 57/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2285 - acc: 0.9044\n",
      "Epoch 58/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2281 - acc: 0.9034\n",
      "Epoch 59/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2285 - acc: 0.9043\n",
      "Epoch 60/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2280 - acc: 0.9039\n",
      "Epoch 61/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2280 - acc: 0.9035\n",
      "Epoch 62/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2283 - acc: 0.9034: 1s - los\n",
      "Epoch 63/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2283 - acc: 0.9044\n",
      "Epoch 64/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2284 - acc: 0.9051\n",
      "Epoch 65/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2278 - acc: 0.9041\n",
      "Epoch 66/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2280 - acc: 0.9046\n",
      "Epoch 67/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2273 - acc: 0.9038\n",
      "Epoch 68/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2276 - acc: 0.9044\n",
      "Epoch 69/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2273 - acc: 0.9033\n",
      "Epoch 70/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2283 - acc: 0.9039\n",
      "Epoch 71/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2276 - acc: 0.9044\n",
      "Epoch 72/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2275 - acc: 0.9052\n",
      "Epoch 73/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2275 - acc: 0.9047\n",
      "Epoch 74/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2271 - acc: 0.9047\n",
      "Epoch 75/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2274 - acc: 0.9036\n",
      "Epoch 76/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2276 - acc: 0.9041\n",
      "Epoch 77/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2277 - acc: 0.9038\n",
      "Epoch 78/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2278 - acc: 0.9039\n",
      "Epoch 79/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2272 - acc: 0.9046\n",
      "Epoch 80/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2273 - acc: 0.9047\n",
      "Epoch 81/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2272 - acc: 0.9036\n",
      "Epoch 82/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2274 - acc: 0.9043\n",
      "Epoch 83/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2276 - acc: 0.9053\n",
      "Epoch 84/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2276 - acc: 0.9051\n",
      "Epoch 85/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2268 - acc: 0.9054\n",
      "Epoch 86/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2269 - acc: 0.9044\n",
      "Epoch 87/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2272 - acc: 0.9041\n",
      "Epoch 88/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2269 - acc: 0.9048\n",
      "Epoch 89/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2275 - acc: 0.9041\n",
      "Epoch 90/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2270 - acc: 0.9048\n",
      "Epoch 91/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2267 - acc: 0.9051\n",
      "Epoch 92/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2265 - acc: 0.9052\n",
      "Epoch 93/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2268 - acc: 0.9058\n",
      "Epoch 94/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2268 - acc: 0.9056\n",
      "Epoch 95/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2267 - acc: 0.9047\n",
      "Epoch 96/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2266 - acc: 0.9051\n",
      "Epoch 97/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2265 - acc: 0.9042\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2266 - acc: 0.9053\n",
      "Epoch 99/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2268 - acc: 0.9052\n",
      "Epoch 100/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2266 - acc: 0.9029\n",
      "2893/2893 [==============================] - 0s 113us/step\n",
      "Epoch 1/100\n",
      "17360/17360 [==============================] - 2s 116us/step - loss: 0.3225 - acc: 0.8816\n",
      "Epoch 2/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2624 - acc: 0.8937\n",
      "Epoch 3/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2553 - acc: 0.8927\n",
      "Epoch 4/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2513 - acc: 0.8951\n",
      "Epoch 5/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2481 - acc: 0.8958\n",
      "Epoch 6/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2464 - acc: 0.8975\n",
      "Epoch 7/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2440 - acc: 0.8995\n",
      "Epoch 8/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2416 - acc: 0.8982\n",
      "Epoch 9/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2401 - acc: 0.9001\n",
      "Epoch 10/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2392 - acc: 0.9002\n",
      "Epoch 11/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2384 - acc: 0.9007\n",
      "Epoch 12/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2376 - acc: 0.9014\n",
      "Epoch 13/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2375 - acc: 0.9028\n",
      "Epoch 14/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2365 - acc: 0.9017\n",
      "Epoch 15/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2367 - acc: 0.9024\n",
      "Epoch 16/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2366 - acc: 0.9031\n",
      "Epoch 17/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2360 - acc: 0.9029\n",
      "Epoch 18/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2354 - acc: 0.9031\n",
      "Epoch 19/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2350 - acc: 0.9024\n",
      "Epoch 20/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2353 - acc: 0.9029\n",
      "Epoch 21/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2351 - acc: 0.9027\n",
      "Epoch 22/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2350 - acc: 0.9028\n",
      "Epoch 23/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2343 - acc: 0.9031\n",
      "Epoch 24/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2344 - acc: 0.9027\n",
      "Epoch 25/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2339 - acc: 0.9031\n",
      "Epoch 26/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2346 - acc: 0.9022\n",
      "Epoch 27/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2337 - acc: 0.9028\n",
      "Epoch 28/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2332 - acc: 0.9042\n",
      "Epoch 29/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2334 - acc: 0.9039\n",
      "Epoch 30/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2326 - acc: 0.9024\n",
      "Epoch 31/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2329 - acc: 0.9031: 0s - loss: 0.2329 - acc: 0.90\n",
      "Epoch 32/100\n",
      "17360/17360 [==============================] - 1s 66us/step - loss: 0.2326 - acc: 0.9022\n",
      "Epoch 33/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2321 - acc: 0.9040\n",
      "Epoch 34/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2326 - acc: 0.9024\n",
      "Epoch 35/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2324 - acc: 0.9032\n",
      "Epoch 36/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2321 - acc: 0.9037\n",
      "Epoch 37/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2319 - acc: 0.9036\n",
      "Epoch 38/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2320 - acc: 0.9025\n",
      "Epoch 39/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2318 - acc: 0.9029\n",
      "Epoch 40/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2311 - acc: 0.9048\n",
      "Epoch 41/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2308 - acc: 0.9037\n",
      "Epoch 42/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2311 - acc: 0.9028\n",
      "Epoch 43/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2307 - acc: 0.9033\n",
      "Epoch 44/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2305 - acc: 0.9049\n",
      "Epoch 45/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2302 - acc: 0.9040\n",
      "Epoch 46/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2302 - acc: 0.9046\n",
      "Epoch 47/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2305 - acc: 0.9041\n",
      "Epoch 48/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2304 - acc: 0.9038\n",
      "Epoch 49/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2303 - acc: 0.9055\n",
      "Epoch 50/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2301 - acc: 0.9043\n",
      "Epoch 51/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2294 - acc: 0.9038\n",
      "Epoch 52/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2298 - acc: 0.9043\n",
      "Epoch 53/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2298 - acc: 0.9050\n",
      "Epoch 54/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2299 - acc: 0.9048\n",
      "Epoch 55/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2292 - acc: 0.9046\n",
      "Epoch 56/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2289 - acc: 0.9038\n",
      "Epoch 57/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2295 - acc: 0.9047\n",
      "Epoch 58/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2297 - acc: 0.9050\n",
      "Epoch 59/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2294 - acc: 0.9048\n",
      "Epoch 60/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2288 - acc: 0.9036\n",
      "Epoch 61/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2288 - acc: 0.9052\n",
      "Epoch 62/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2287 - acc: 0.9043\n",
      "Epoch 63/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2288 - acc: 0.9041\n",
      "Epoch 64/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2286 - acc: 0.9047: 1s - loss: \n",
      "Epoch 65/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2288 - acc: 0.9048\n",
      "Epoch 66/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2287 - acc: 0.9060\n",
      "Epoch 67/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2279 - acc: 0.9062\n",
      "Epoch 68/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2286 - acc: 0.9051\n",
      "Epoch 69/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2284 - acc: 0.9057\n",
      "Epoch 70/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2278 - acc: 0.9060\n",
      "Epoch 71/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2286 - acc: 0.9057\n",
      "Epoch 72/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2282 - acc: 0.9055\n",
      "Epoch 73/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2284 - acc: 0.9056\n",
      "Epoch 74/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2282 - acc: 0.9052\n",
      "Epoch 75/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2275 - acc: 0.9059\n",
      "Epoch 76/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2283 - acc: 0.9051\n",
      "Epoch 77/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2281 - acc: 0.9047\n",
      "Epoch 78/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2282 - acc: 0.9057\n",
      "Epoch 79/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2277 - acc: 0.9063\n",
      "Epoch 80/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2277 - acc: 0.9049\n",
      "Epoch 81/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2279 - acc: 0.9052\n",
      "Epoch 82/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2275 - acc: 0.9048\n",
      "Epoch 83/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2276 - acc: 0.9068\n",
      "Epoch 84/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2276 - acc: 0.9059\n",
      "Epoch 85/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2280 - acc: 0.9047\n",
      "Epoch 86/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2276 - acc: 0.9063\n",
      "Epoch 87/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2278 - acc: 0.9055\n",
      "Epoch 88/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2279 - acc: 0.9058\n",
      "Epoch 89/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2273 - acc: 0.9054\n",
      "Epoch 90/100\n",
      "17360/17360 [==============================] - 1s 67us/step - loss: 0.2276 - acc: 0.9050\n",
      "Epoch 91/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2276 - acc: 0.9054\n",
      "Epoch 92/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2271 - acc: 0.9055\n",
      "Epoch 93/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2274 - acc: 0.9050\n",
      "Epoch 94/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2271 - acc: 0.9057\n",
      "Epoch 95/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2275 - acc: 0.9060\n",
      "Epoch 96/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2274 - acc: 0.9050\n",
      "Epoch 97/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2269 - acc: 0.9056\n",
      "Epoch 98/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2272 - acc: 0.9050\n",
      "Epoch 99/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2273 - acc: 0.9054\n",
      "Epoch 100/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2266 - acc: 0.9049\n",
      "2893/2893 [==============================] - 0s 118us/step\n",
      "Epoch 1/100\n",
      "17360/17360 [==============================] - 2s 114us/step - loss: 0.3152 - acc: 0.8866\n",
      "Epoch 2/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2628 - acc: 0.8919\n",
      "Epoch 3/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2549 - acc: 0.8929\n",
      "Epoch 4/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2506 - acc: 0.8941\n",
      "Epoch 5/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2479 - acc: 0.8955\n",
      "Epoch 6/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2450 - acc: 0.8970\n",
      "Epoch 7/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2427 - acc: 0.8975\n",
      "Epoch 8/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2428 - acc: 0.8978\n",
      "Epoch 9/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2413 - acc: 0.8998\n",
      "Epoch 10/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2408 - acc: 0.9003\n",
      "Epoch 11/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2397 - acc: 0.9001\n",
      "Epoch 12/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2398 - acc: 0.9003\n",
      "Epoch 13/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2394 - acc: 0.9009\n",
      "Epoch 14/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2385 - acc: 0.9005\n",
      "Epoch 15/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2384 - acc: 0.9009\n",
      "Epoch 16/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2383 - acc: 0.8993\n",
      "Epoch 17/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2376 - acc: 0.9003\n",
      "Epoch 18/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2380 - acc: 0.9006\n",
      "Epoch 19/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2364 - acc: 0.9008\n",
      "Epoch 20/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2371 - acc: 0.9009\n",
      "Epoch 21/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2365 - acc: 0.9006\n",
      "Epoch 22/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2370 - acc: 0.8995\n",
      "Epoch 23/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2365 - acc: 0.9001\n",
      "Epoch 24/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2355 - acc: 0.9008\n",
      "Epoch 25/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2359 - acc: 0.9005\n",
      "Epoch 26/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2356 - acc: 0.9007\n",
      "Epoch 27/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2344 - acc: 0.9014\n",
      "Epoch 28/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2344 - acc: 0.9013\n",
      "Epoch 29/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2340 - acc: 0.9002\n",
      "Epoch 30/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2345 - acc: 0.9010\n",
      "Epoch 31/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2338 - acc: 0.9006\n",
      "Epoch 32/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2343 - acc: 0.9017: 1s - loss:\n",
      "Epoch 33/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2335 - acc: 0.9012\n",
      "Epoch 34/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2340 - acc: 0.9018\n",
      "Epoch 35/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2336 - acc: 0.9014\n",
      "Epoch 36/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2328 - acc: 0.9019\n",
      "Epoch 37/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2330 - acc: 0.9026\n",
      "Epoch 38/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2325 - acc: 0.9020\n",
      "Epoch 39/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2321 - acc: 0.9024\n",
      "Epoch 40/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2321 - acc: 0.9028\n",
      "Epoch 41/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2324 - acc: 0.9024\n",
      "Epoch 42/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2326 - acc: 0.9021\n",
      "Epoch 43/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2318 - acc: 0.9017\n",
      "Epoch 44/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2317 - acc: 0.9024\n",
      "Epoch 45/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2309 - acc: 0.9029\n",
      "Epoch 46/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2315 - acc: 0.9020\n",
      "Epoch 47/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2310 - acc: 0.9021\n",
      "Epoch 48/100\n",
      "17360/17360 [==============================] - 1s 81us/step - loss: 0.2313 - acc: 0.9032\n",
      "Epoch 49/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2313 - acc: 0.9030\n",
      "Epoch 50/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2313 - acc: 0.9021\n",
      "Epoch 51/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2304 - acc: 0.9021\n",
      "Epoch 52/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2308 - acc: 0.9034\n",
      "Epoch 53/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2304 - acc: 0.9018\n",
      "Epoch 54/100\n",
      "17360/17360 [==============================] - 1s 78us/step - loss: 0.2306 - acc: 0.9027\n",
      "Epoch 55/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2298 - acc: 0.9025\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2297 - acc: 0.9031\n",
      "Epoch 57/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2295 - acc: 0.9019\n",
      "Epoch 58/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2308 - acc: 0.9032\n",
      "Epoch 59/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2301 - acc: 0.9029\n",
      "Epoch 60/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2298 - acc: 0.9028\n",
      "Epoch 61/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2300 - acc: 0.9025\n",
      "Epoch 62/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2301 - acc: 0.9023\n",
      "Epoch 63/100\n",
      "17360/17360 [==============================] - 1s 78us/step - loss: 0.2292 - acc: 0.9028\n",
      "Epoch 64/100\n",
      "17360/17360 [==============================] - 1s 77us/step - loss: 0.2298 - acc: 0.9025\n",
      "Epoch 65/100\n",
      "17360/17360 [==============================] - 2s 87us/step - loss: 0.2296 - acc: 0.9022\n",
      "Epoch 66/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2300 - acc: 0.9028\n",
      "Epoch 67/100\n",
      "17360/17360 [==============================] - 1s 76us/step - loss: 0.2290 - acc: 0.9025\n",
      "Epoch 68/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2292 - acc: 0.9034\n",
      "Epoch 69/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2292 - acc: 0.9022\n",
      "Epoch 70/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2295 - acc: 0.9024\n",
      "Epoch 71/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2292 - acc: 0.9027\n",
      "Epoch 72/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2295 - acc: 0.9032\n",
      "Epoch 73/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2287 - acc: 0.9026\n",
      "Epoch 74/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2282 - acc: 0.9036\n",
      "Epoch 75/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2287 - acc: 0.9021\n",
      "Epoch 76/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2292 - acc: 0.9034\n",
      "Epoch 77/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2290 - acc: 0.9033\n",
      "Epoch 78/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2279 - acc: 0.9017\n",
      "Epoch 79/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2280 - acc: 0.9029\n",
      "Epoch 80/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2286 - acc: 0.9030\n",
      "Epoch 81/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2288 - acc: 0.9035\n",
      "Epoch 82/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2281 - acc: 0.9033\n",
      "Epoch 83/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2290 - acc: 0.9026\n",
      "Epoch 84/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2288 - acc: 0.9033\n",
      "Epoch 85/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2276 - acc: 0.9035\n",
      "Epoch 86/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2280 - acc: 0.9027\n",
      "Epoch 87/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2286 - acc: 0.9027\n",
      "Epoch 88/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2278 - acc: 0.9038\n",
      "Epoch 89/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2282 - acc: 0.9029\n",
      "Epoch 90/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2276 - acc: 0.9032\n",
      "Epoch 91/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2279 - acc: 0.9041\n",
      "Epoch 92/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2287 - acc: 0.9037\n",
      "Epoch 93/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2273 - acc: 0.9044\n",
      "Epoch 94/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2272 - acc: 0.9029\n",
      "Epoch 95/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2284 - acc: 0.9033: 1s - loss\n",
      "Epoch 96/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2270 - acc: 0.9032\n",
      "Epoch 97/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2271 - acc: 0.9029\n",
      "Epoch 98/100\n",
      "17360/17360 [==============================] - 1s 80us/step - loss: 0.2274 - acc: 0.9029\n",
      "Epoch 99/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2275 - acc: 0.9031\n",
      "Epoch 100/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2275 - acc: 0.9037\n",
      "2893/2893 [==============================] - 0s 130us/step\n",
      "Epoch 1/100\n",
      "17360/17360 [==============================] - 2s 131us/step - loss: 0.3273 - acc: 0.8815\n",
      "Epoch 2/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2615 - acc: 0.8929\n",
      "Epoch 3/100\n",
      "17360/17360 [==============================] - 1s 80us/step - loss: 0.2522 - acc: 0.8950\n",
      "Epoch 4/100\n",
      "17360/17360 [==============================] - 1s 78us/step - loss: 0.2490 - acc: 0.8960\n",
      "Epoch 5/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2462 - acc: 0.8963\n",
      "Epoch 6/100\n",
      "17360/17360 [==============================] - 1s 80us/step - loss: 0.2446 - acc: 0.8979\n",
      "Epoch 7/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2429 - acc: 0.8982\n",
      "Epoch 8/100\n",
      "17360/17360 [==============================] - 1s 81us/step - loss: 0.2417 - acc: 0.8980\n",
      "Epoch 9/100\n",
      "17360/17360 [==============================] - 1s 79us/step - loss: 0.2407 - acc: 0.8976\n",
      "Epoch 10/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2404 - acc: 0.8985\n",
      "Epoch 11/100\n",
      "17360/17360 [==============================] - 1s 84us/step - loss: 0.2393 - acc: 0.9009\n",
      "Epoch 12/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2393 - acc: 0.8999\n",
      "Epoch 13/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2387 - acc: 0.9003\n",
      "Epoch 14/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2386 - acc: 0.9003\n",
      "Epoch 15/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2382 - acc: 0.9005\n",
      "Epoch 16/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2376 - acc: 0.9007\n",
      "Epoch 17/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2376 - acc: 0.9005\n",
      "Epoch 18/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2374 - acc: 0.8996\n",
      "Epoch 19/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2366 - acc: 0.9011\n",
      "Epoch 20/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2362 - acc: 0.9002\n",
      "Epoch 21/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2362 - acc: 0.9001\n",
      "Epoch 22/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2361 - acc: 0.9006\n",
      "Epoch 23/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2358 - acc: 0.9009\n",
      "Epoch 24/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2350 - acc: 0.9024\n",
      "Epoch 25/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2353 - acc: 0.9009\n",
      "Epoch 26/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2343 - acc: 0.9006\n",
      "Epoch 27/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2341 - acc: 0.9002\n",
      "Epoch 28/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2338 - acc: 0.9017\n",
      "Epoch 29/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2334 - acc: 0.9017: 0s - loss: 0.2331 - ac\n",
      "Epoch 30/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2333 - acc: 0.9006\n",
      "Epoch 31/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2327 - acc: 0.9010\n",
      "Epoch 32/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2330 - acc: 0.9014\n",
      "Epoch 33/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2325 - acc: 0.9007\n",
      "Epoch 34/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2323 - acc: 0.9002\n",
      "Epoch 35/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2324 - acc: 0.9003\n",
      "Epoch 36/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2323 - acc: 0.9018\n",
      "Epoch 37/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2322 - acc: 0.9006\n",
      "Epoch 38/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2319 - acc: 0.9009\n",
      "Epoch 39/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2322 - acc: 0.9020\n",
      "Epoch 40/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2314 - acc: 0.9012\n",
      "Epoch 41/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2317 - acc: 0.9002\n",
      "Epoch 42/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2310 - acc: 0.9006\n",
      "Epoch 43/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2311 - acc: 0.9009\n",
      "Epoch 44/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2315 - acc: 0.9017\n",
      "Epoch 45/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2313 - acc: 0.9001\n",
      "Epoch 46/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2308 - acc: 0.9002\n",
      "Epoch 47/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2304 - acc: 0.9014\n",
      "Epoch 48/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2308 - acc: 0.9013\n",
      "Epoch 49/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2307 - acc: 0.9009\n",
      "Epoch 50/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2304 - acc: 0.9003\n",
      "Epoch 51/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2304 - acc: 0.9026\n",
      "Epoch 52/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2298 - acc: 0.9006\n",
      "Epoch 53/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2301 - acc: 0.9003\n",
      "Epoch 54/100\n",
      "17360/17360 [==============================] - 1s 68us/step - loss: 0.2300 - acc: 0.9011\n",
      "Epoch 55/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2296 - acc: 0.9019\n",
      "Epoch 56/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2298 - acc: 0.9014\n",
      "Epoch 57/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2293 - acc: 0.9031\n",
      "Epoch 58/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2295 - acc: 0.9025\n",
      "Epoch 59/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2296 - acc: 0.9010\n",
      "Epoch 60/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2291 - acc: 0.9012\n",
      "Epoch 61/100\n",
      "17360/17360 [==============================] - 1s 78us/step - loss: 0.2292 - acc: 0.9018\n",
      "Epoch 62/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2290 - acc: 0.9012\n",
      "Epoch 63/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2288 - acc: 0.9018\n",
      "Epoch 64/100\n",
      "17360/17360 [==============================] - 1s 81us/step - loss: 0.2285 - acc: 0.9017\n",
      "Epoch 65/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2290 - acc: 0.9013\n",
      "Epoch 66/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2280 - acc: 0.9029\n",
      "Epoch 67/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2283 - acc: 0.9022\n",
      "Epoch 68/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2289 - acc: 0.9016\n",
      "Epoch 69/100\n",
      "17360/17360 [==============================] - 1s 75us/step - loss: 0.2284 - acc: 0.9024\n",
      "Epoch 70/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2281 - acc: 0.9018\n",
      "Epoch 71/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2273 - acc: 0.9029\n",
      "Epoch 72/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2277 - acc: 0.9020\n",
      "Epoch 73/100\n",
      "17360/17360 [==============================] - 1s 77us/step - loss: 0.2277 - acc: 0.9010\n",
      "Epoch 74/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2281 - acc: 0.9012\n",
      "Epoch 75/100\n",
      "17360/17360 [==============================] - 1s 80us/step - loss: 0.2277 - acc: 0.9019\n",
      "Epoch 76/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2277 - acc: 0.9026\n",
      "Epoch 77/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2270 - acc: 0.9028\n",
      "Epoch 78/100\n",
      "17360/17360 [==============================] - 1s 80us/step - loss: 0.2272 - acc: 0.9029\n",
      "Epoch 79/100\n",
      "17360/17360 [==============================] - 1s 77us/step - loss: 0.2272 - acc: 0.9022\n",
      "Epoch 80/100\n",
      "17360/17360 [==============================] - 1s 74us/step - loss: 0.2273 - acc: 0.9022\n",
      "Epoch 81/100\n",
      "17360/17360 [==============================] - 1s 78us/step - loss: 0.2265 - acc: 0.9022\n",
      "Epoch 82/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2272 - acc: 0.9033\n",
      "Epoch 83/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2256 - acc: 0.9028\n",
      "Epoch 84/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2261 - acc: 0.9027\n",
      "Epoch 85/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2261 - acc: 0.9026\n",
      "Epoch 86/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2265 - acc: 0.9029\n",
      "Epoch 87/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2265 - acc: 0.9027\n",
      "Epoch 88/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2261 - acc: 0.9029: 1s - lo\n",
      "Epoch 89/100\n",
      "17360/17360 [==============================] - 1s 73us/step - loss: 0.2265 - acc: 0.9018\n",
      "Epoch 90/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2262 - acc: 0.9033\n",
      "Epoch 91/100\n",
      "17360/17360 [==============================] - 1s 72us/step - loss: 0.2264 - acc: 0.9025\n",
      "Epoch 92/100\n",
      "17360/17360 [==============================] - 1s 71us/step - loss: 0.2262 - acc: 0.9023\n",
      "Epoch 93/100\n",
      "17360/17360 [==============================] - 1s 83us/step - loss: 0.2261 - acc: 0.9028\n",
      "Epoch 94/100\n",
      "17360/17360 [==============================] - 2s 92us/step - loss: 0.2255 - acc: 0.9036\n",
      "Epoch 95/100\n",
      "17360/17360 [==============================] - 1s 85us/step - loss: 0.2255 - acc: 0.9030\n",
      "Epoch 96/100\n",
      "17360/17360 [==============================] - 1s 79us/step - loss: 0.2262 - acc: 0.9035\n",
      "Epoch 97/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2257 - acc: 0.9031\n",
      "Epoch 98/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2254 - acc: 0.9023\n",
      "Epoch 99/100\n",
      "17360/17360 [==============================] - 1s 69us/step - loss: 0.2258 - acc: 0.9029\n",
      "Epoch 100/100\n",
      "17360/17360 [==============================] - 1s 70us/step - loss: 0.2252 - acc: 0.9032\n",
      "2893/2893 [==============================] - 0s 136us/step\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=7, shuffle=True, random_state=7)\n",
    "k_model = KerasClassifier(build_fn=create_model_fcn, epochs=100, batch_size=20, verbose=1)\n",
    "results = cross_val_score(k_model, x_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_model = Sequential()\n",
    "units_list = [16]\n",
    "init = 'normal'\n",
    "optimizer = 'adam'\n",
    "units = units_list[0]\n",
    "fcn_model.add(Dense(units=units, activation='relu', input_dim=16, kernel_initializer=init))\n",
    "for units in units_list[1:]:\n",
    "    fcn_model.add(Dense(units=units, activation='relu', kernel_initializer=init))\n",
    "fcn_model.add(Dense(units=1, activation='sigmoid', kernel_initializer=init))\n",
    "fcn_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20253 samples, validate on 5064 samples\n",
      "Epoch 1/500\n",
      "20253/20253 [==============================] - 2s 93us/step - loss: 0.3444 - acc: 0.8807 - val_loss: 0.2978 - val_acc: 0.8831\n",
      "Epoch 2/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2703 - acc: 0.8904 - val_loss: 0.2655 - val_acc: 0.8906\n",
      "Epoch 3/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2574 - acc: 0.8933 - val_loss: 0.2635 - val_acc: 0.8896\n",
      "Epoch 4/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2532 - acc: 0.8929 - val_loss: 0.2567 - val_acc: 0.8930\n",
      "Epoch 5/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2504 - acc: 0.8951 - val_loss: 0.2534 - val_acc: 0.8938\n",
      "Epoch 6/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2475 - acc: 0.8956 - val_loss: 0.2539 - val_acc: 0.8955\n",
      "Epoch 7/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2457 - acc: 0.8964 - val_loss: 0.2500 - val_acc: 0.8959\n",
      "Epoch 8/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2429 - acc: 0.8985 - val_loss: 0.2499 - val_acc: 0.8963\n",
      "Epoch 9/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2426 - acc: 0.8986 - val_loss: 0.2480 - val_acc: 0.8973\n",
      "Epoch 10/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2411 - acc: 0.8986 - val_loss: 0.2479 - val_acc: 0.8977\n",
      "Epoch 11/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2400 - acc: 0.8996 - val_loss: 0.2458 - val_acc: 0.8995\n",
      "Epoch 12/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2395 - acc: 0.9005 - val_loss: 0.2462 - val_acc: 0.8975\n",
      "Epoch 13/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2386 - acc: 0.9003 - val_loss: 0.2453 - val_acc: 0.9003\n",
      "Epoch 14/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2384 - acc: 0.9011 - val_loss: 0.2450 - val_acc: 0.8989\n",
      "Epoch 15/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2381 - acc: 0.9012 - val_loss: 0.2447 - val_acc: 0.8973\n",
      "Epoch 16/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2376 - acc: 0.9009 - val_loss: 0.2447 - val_acc: 0.8995\n",
      "Epoch 17/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2363 - acc: 0.9010 - val_loss: 0.2442 - val_acc: 0.8995\n",
      "Epoch 18/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2368 - acc: 0.9013 - val_loss: 0.2463 - val_acc: 0.8973\n",
      "Epoch 19/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2369 - acc: 0.9013 - val_loss: 0.2454 - val_acc: 0.8973\n",
      "Epoch 20/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2363 - acc: 0.9015 - val_loss: 0.2446 - val_acc: 0.8995\n",
      "Epoch 21/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2359 - acc: 0.9019 - val_loss: 0.2439 - val_acc: 0.8997\n",
      "Epoch 22/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2360 - acc: 0.9018 - val_loss: 0.2434 - val_acc: 0.8999\n",
      "Epoch 23/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2353 - acc: 0.9020 - val_loss: 0.2452 - val_acc: 0.8997\n",
      "Epoch 24/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2347 - acc: 0.9018 - val_loss: 0.2436 - val_acc: 0.8983\n",
      "Epoch 25/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2353 - acc: 0.9026 - val_loss: 0.2453 - val_acc: 0.8973\n",
      "Epoch 26/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2353 - acc: 0.9024 - val_loss: 0.2424 - val_acc: 0.8987\n",
      "Epoch 27/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2340 - acc: 0.9022 - val_loss: 0.2428 - val_acc: 0.8999\n",
      "Epoch 28/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2339 - acc: 0.9019 - val_loss: 0.2432 - val_acc: 0.8991\n",
      "Epoch 29/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2337 - acc: 0.9032 - val_loss: 0.2486 - val_acc: 0.8987\n",
      "Epoch 30/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2343 - acc: 0.9029 - val_loss: 0.2428 - val_acc: 0.8985\n",
      "Epoch 31/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2333 - acc: 0.9036 - val_loss: 0.2427 - val_acc: 0.8975\n",
      "Epoch 32/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2341 - acc: 0.9012 - val_loss: 0.2427 - val_acc: 0.9011\n",
      "Epoch 33/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2329 - acc: 0.9027 - val_loss: 0.2435 - val_acc: 0.8993\n",
      "Epoch 34/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2333 - acc: 0.9023 - val_loss: 0.2413 - val_acc: 0.9017\n",
      "Epoch 35/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2325 - acc: 0.9032 - val_loss: 0.2422 - val_acc: 0.8981\n",
      "Epoch 36/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2328 - acc: 0.9020 - val_loss: 0.2417 - val_acc: 0.8985\n",
      "Epoch 37/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2321 - acc: 0.9027 - val_loss: 0.2415 - val_acc: 0.8999\n",
      "Epoch 38/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2322 - acc: 0.9018 - val_loss: 0.2527 - val_acc: 0.9009\n",
      "Epoch 39/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2323 - acc: 0.9025 - val_loss: 0.2419 - val_acc: 0.9015\n",
      "Epoch 40/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2321 - acc: 0.9028 - val_loss: 0.2415 - val_acc: 0.9009\n",
      "Epoch 41/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2317 - acc: 0.9027 - val_loss: 0.2420 - val_acc: 0.9015\n",
      "Epoch 42/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2318 - acc: 0.9019 - val_loss: 0.2404 - val_acc: 0.9023\n",
      "Epoch 43/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2310 - acc: 0.9029 - val_loss: 0.2421 - val_acc: 0.9011\n",
      "Epoch 44/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2310 - acc: 0.9022 - val_loss: 0.2410 - val_acc: 0.9009\n",
      "Epoch 45/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2312 - acc: 0.9018 - val_loss: 0.2416 - val_acc: 0.9003\n",
      "Epoch 46/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2311 - acc: 0.9020 - val_loss: 0.2407 - val_acc: 0.8983\n",
      "Epoch 47/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2311 - acc: 0.9021 - val_loss: 0.2407 - val_acc: 0.9011\n",
      "Epoch 48/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2307 - acc: 0.9029 - val_loss: 0.2405 - val_acc: 0.8997\n",
      "Epoch 49/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2310 - acc: 0.9025 - val_loss: 0.2408 - val_acc: 0.9007\n",
      "Epoch 50/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2305 - acc: 0.9029 - val_loss: 0.2401 - val_acc: 0.9017\n",
      "Epoch 51/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2305 - acc: 0.9029 - val_loss: 0.2502 - val_acc: 0.8987\n",
      "Epoch 52/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2311 - acc: 0.9026 - val_loss: 0.2409 - val_acc: 0.9003\n",
      "Epoch 53/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2298 - acc: 0.9019 - val_loss: 0.2426 - val_acc: 0.9005\n",
      "Epoch 54/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2299 - acc: 0.9028 - val_loss: 0.2425 - val_acc: 0.8993\n",
      "Epoch 55/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2299 - acc: 0.9020 - val_loss: 0.2401 - val_acc: 0.8997\n",
      "Epoch 56/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2304 - acc: 0.9015 - val_loss: 0.2408 - val_acc: 0.9021\n",
      "Epoch 57/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2293 - acc: 0.9027 - val_loss: 0.2401 - val_acc: 0.9017\n",
      "Epoch 58/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2296 - acc: 0.9021 - val_loss: 0.2408 - val_acc: 0.8995\n",
      "Epoch 59/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2292 - acc: 0.9043 - val_loss: 0.2403 - val_acc: 0.8997\n",
      "Epoch 60/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2302 - acc: 0.9029 - val_loss: 0.2404 - val_acc: 0.9011\n",
      "Epoch 61/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2294 - acc: 0.9026 - val_loss: 0.2399 - val_acc: 0.8989\n",
      "Epoch 62/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2295 - acc: 0.9034 - val_loss: 0.2410 - val_acc: 0.8999\n",
      "Epoch 63/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2290 - acc: 0.9031 - val_loss: 0.2392 - val_acc: 0.9003\n",
      "Epoch 64/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2287 - acc: 0.9029 - val_loss: 0.2406 - val_acc: 0.9015\n",
      "Epoch 65/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2287 - acc: 0.9040 - val_loss: 0.2401 - val_acc: 0.8993\n",
      "Epoch 66/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2284 - acc: 0.9029 - val_loss: 0.2401 - val_acc: 0.9015\n",
      "Epoch 67/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2279 - acc: 0.9031 - val_loss: 0.2389 - val_acc: 0.8999\n",
      "Epoch 68/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2284 - acc: 0.9030 - val_loss: 0.2394 - val_acc: 0.9023\n",
      "Epoch 69/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2282 - acc: 0.9029 - val_loss: 0.2391 - val_acc: 0.9007\n",
      "Epoch 70/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2276 - acc: 0.9030 - val_loss: 0.2428 - val_acc: 0.9013\n",
      "Epoch 71/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2275 - acc: 0.9035 - val_loss: 0.2387 - val_acc: 0.9030\n",
      "Epoch 72/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2275 - acc: 0.9037 - val_loss: 0.2398 - val_acc: 0.9017\n",
      "Epoch 73/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2276 - acc: 0.9041 - val_loss: 0.2412 - val_acc: 0.9021\n",
      "Epoch 74/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2278 - acc: 0.9022 - val_loss: 0.2387 - val_acc: 0.9024\n",
      "Epoch 75/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2271 - acc: 0.9031 - val_loss: 0.2391 - val_acc: 0.9007\n",
      "Epoch 76/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2270 - acc: 0.9037 - val_loss: 0.2410 - val_acc: 0.9028\n",
      "Epoch 77/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2275 - acc: 0.9031 - val_loss: 0.2394 - val_acc: 0.9028\n",
      "Epoch 78/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2277 - acc: 0.9040 - val_loss: 0.2383 - val_acc: 0.9030\n",
      "Epoch 79/500\n",
      "20253/20253 [==============================] - 1s 29us/step - loss: 0.2265 - acc: 0.9027 - val_loss: 0.2410 - val_acc: 0.9026\n",
      "Epoch 80/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2266 - acc: 0.9051 - val_loss: 0.2384 - val_acc: 0.9024\n",
      "Epoch 81/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2269 - acc: 0.9043 - val_loss: 0.2381 - val_acc: 0.9034\n",
      "Epoch 82/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2267 - acc: 0.9025 - val_loss: 0.2409 - val_acc: 0.9009\n",
      "Epoch 83/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2270 - acc: 0.9041 - val_loss: 0.2378 - val_acc: 0.9026\n",
      "Epoch 84/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2263 - acc: 0.9044 - val_loss: 0.2449 - val_acc: 0.9023\n",
      "Epoch 85/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2264 - acc: 0.9042 - val_loss: 0.2442 - val_acc: 0.9028\n",
      "Epoch 86/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2274 - acc: 0.9038 - val_loss: 0.2416 - val_acc: 0.9028\n",
      "Epoch 87/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2276 - acc: 0.9037 - val_loss: 0.2376 - val_acc: 0.9042\n",
      "Epoch 88/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2267 - acc: 0.9043 - val_loss: 0.2433 - val_acc: 0.9026\n",
      "Epoch 89/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2265 - acc: 0.9037 - val_loss: 0.2407 - val_acc: 0.9026\n",
      "Epoch 90/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2276 - acc: 0.9037 - val_loss: 0.2397 - val_acc: 0.9011\n",
      "Epoch 91/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2263 - acc: 0.9039 - val_loss: 0.2378 - val_acc: 0.9030\n",
      "Epoch 92/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2261 - acc: 0.9035 - val_loss: 0.2403 - val_acc: 0.9028\n",
      "Epoch 93/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2269 - acc: 0.9038 - val_loss: 0.2378 - val_acc: 0.9024\n",
      "Epoch 94/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2264 - acc: 0.9043 - val_loss: 0.2382 - val_acc: 0.9023\n",
      "Epoch 95/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2260 - acc: 0.9038 - val_loss: 0.2378 - val_acc: 0.9019\n",
      "Epoch 96/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2258 - acc: 0.9039 - val_loss: 0.2377 - val_acc: 0.9023\n",
      "Epoch 97/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2260 - acc: 0.9037 - val_loss: 0.2387 - val_acc: 0.9032\n",
      "Epoch 98/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2267 - acc: 0.9043 - val_loss: 0.2377 - val_acc: 0.9024\n",
      "Epoch 99/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2260 - acc: 0.9044 - val_loss: 0.2390 - val_acc: 0.9021\n",
      "Epoch 100/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2263 - acc: 0.9038 - val_loss: 0.2387 - val_acc: 0.9021\n",
      "Epoch 101/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2261 - acc: 0.9030 - val_loss: 0.2379 - val_acc: 0.9032\n",
      "Epoch 102/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2260 - acc: 0.9043 - val_loss: 0.2415 - val_acc: 0.9011\n",
      "Epoch 103/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2257 - acc: 0.9039 - val_loss: 0.2372 - val_acc: 0.9023\n",
      "Epoch 104/500\n",
      "20253/20253 [==============================] - 1s 29us/step - loss: 0.2258 - acc: 0.9042 - val_loss: 0.2397 - val_acc: 0.9023\n",
      "Epoch 105/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2258 - acc: 0.9042 - val_loss: 0.2383 - val_acc: 0.9034\n",
      "Epoch 106/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2257 - acc: 0.9036 - val_loss: 0.2427 - val_acc: 0.9023\n",
      "Epoch 107/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2258 - acc: 0.9037 - val_loss: 0.2387 - val_acc: 0.9040\n",
      "Epoch 108/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2261 - acc: 0.9029 - val_loss: 0.2381 - val_acc: 0.9026\n",
      "Epoch 109/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2255 - acc: 0.9042 - val_loss: 0.2387 - val_acc: 0.9026\n",
      "Epoch 110/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2259 - acc: 0.9038 - val_loss: 0.2388 - val_acc: 0.9023\n",
      "Epoch 111/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2258 - acc: 0.9047 - val_loss: 0.2395 - val_acc: 0.9036\n",
      "Epoch 112/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2254 - acc: 0.9038 - val_loss: 0.2410 - val_acc: 0.9030\n",
      "Epoch 113/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2256 - acc: 0.9051 - val_loss: 0.2407 - val_acc: 0.9023\n",
      "Epoch 114/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2249 - acc: 0.9044 - val_loss: 0.2399 - val_acc: 0.9026\n",
      "Epoch 115/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2257 - acc: 0.9041 - val_loss: 0.2411 - val_acc: 0.9038\n",
      "Epoch 116/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2257 - acc: 0.9045 - val_loss: 0.2383 - val_acc: 0.9032\n",
      "Epoch 117/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2250 - acc: 0.9047 - val_loss: 0.2391 - val_acc: 0.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2254 - acc: 0.9035 - val_loss: 0.2390 - val_acc: 0.9017\n",
      "Epoch 119/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2256 - acc: 0.9036 - val_loss: 0.2376 - val_acc: 0.9026\n",
      "Epoch 120/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2256 - acc: 0.9043 - val_loss: 0.2379 - val_acc: 0.9017\n",
      "Epoch 121/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2252 - acc: 0.9033 - val_loss: 0.2409 - val_acc: 0.9032\n",
      "Epoch 122/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2250 - acc: 0.9049 - val_loss: 0.2452 - val_acc: 0.9032\n",
      "Epoch 123/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2255 - acc: 0.9050 - val_loss: 0.2381 - val_acc: 0.9026\n",
      "Epoch 124/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2263 - acc: 0.9034 - val_loss: 0.2409 - val_acc: 0.9024\n",
      "Epoch 125/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2254 - acc: 0.9049 - val_loss: 0.2379 - val_acc: 0.9044\n",
      "Epoch 126/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2257 - acc: 0.9040 - val_loss: 0.2404 - val_acc: 0.9032\n",
      "Epoch 127/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2251 - acc: 0.9043 - val_loss: 0.2386 - val_acc: 0.9030\n",
      "Epoch 128/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2250 - acc: 0.9046 - val_loss: 0.2410 - val_acc: 0.9038\n",
      "Epoch 129/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2259 - acc: 0.9049 - val_loss: 0.2417 - val_acc: 0.9023\n",
      "Epoch 130/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2249 - acc: 0.9055 - val_loss: 0.2385 - val_acc: 0.9021\n",
      "Epoch 131/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2252 - acc: 0.9035 - val_loss: 0.2410 - val_acc: 0.9036\n",
      "Epoch 132/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2248 - acc: 0.9036 - val_loss: 0.2395 - val_acc: 0.9024\n",
      "Epoch 133/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2249 - acc: 0.9043 - val_loss: 0.2382 - val_acc: 0.9034\n",
      "Epoch 134/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2258 - acc: 0.9039 - val_loss: 0.2401 - val_acc: 0.9030\n",
      "Epoch 135/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2246 - acc: 0.9046 - val_loss: 0.2405 - val_acc: 0.9042\n",
      "Epoch 136/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2245 - acc: 0.9047 - val_loss: 0.2411 - val_acc: 0.9032\n",
      "Epoch 137/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2251 - acc: 0.9040 - val_loss: 0.2389 - val_acc: 0.9036\n",
      "Epoch 138/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2250 - acc: 0.9046 - val_loss: 0.2389 - val_acc: 0.9023\n",
      "Epoch 139/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2247 - acc: 0.9040 - val_loss: 0.2389 - val_acc: 0.9024\n",
      "Epoch 140/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2250 - acc: 0.9045 - val_loss: 0.2372 - val_acc: 0.9034\n",
      "Epoch 141/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2253 - acc: 0.9046 - val_loss: 0.2393 - val_acc: 0.9032\n",
      "Epoch 142/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2246 - acc: 0.9050 - val_loss: 0.2375 - val_acc: 0.9019\n",
      "Epoch 143/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2246 - acc: 0.9050 - val_loss: 0.2395 - val_acc: 0.9013\n",
      "Epoch 144/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2241 - acc: 0.9049 - val_loss: 0.2473 - val_acc: 0.9032\n",
      "Epoch 145/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2246 - acc: 0.9040 - val_loss: 0.2409 - val_acc: 0.9030\n",
      "Epoch 146/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2247 - acc: 0.9042 - val_loss: 0.2410 - val_acc: 0.9021\n",
      "Epoch 147/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2242 - acc: 0.9043 - val_loss: 0.2382 - val_acc: 0.9023\n",
      "Epoch 148/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2242 - acc: 0.9045 - val_loss: 0.2394 - val_acc: 0.9032\n",
      "Epoch 149/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2250 - acc: 0.9046 - val_loss: 0.2383 - val_acc: 0.9024\n",
      "Epoch 150/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2245 - acc: 0.9044 - val_loss: 0.2420 - val_acc: 0.9042\n",
      "Epoch 151/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2243 - acc: 0.9045 - val_loss: 0.2380 - val_acc: 0.9028\n",
      "Epoch 152/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2243 - acc: 0.9048 - val_loss: 0.2411 - val_acc: 0.9001\n",
      "Epoch 153/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2244 - acc: 0.9051 - val_loss: 0.2426 - val_acc: 0.9024\n",
      "Epoch 154/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2245 - acc: 0.9041 - val_loss: 0.2376 - val_acc: 0.9024\n",
      "Epoch 155/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2245 - acc: 0.9044 - val_loss: 0.2413 - val_acc: 0.9036\n",
      "Epoch 156/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2242 - acc: 0.9054 - val_loss: 0.2398 - val_acc: 0.9038\n",
      "Epoch 157/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2248 - acc: 0.9051 - val_loss: 0.2378 - val_acc: 0.9015\n",
      "Epoch 158/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2245 - acc: 0.9051 - val_loss: 0.2387 - val_acc: 0.9034\n",
      "Epoch 159/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2238 - acc: 0.9049 - val_loss: 0.2371 - val_acc: 0.9024\n",
      "Epoch 160/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2244 - acc: 0.9051 - val_loss: 0.2388 - val_acc: 0.9021\n",
      "Epoch 161/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2255 - acc: 0.9032 - val_loss: 0.2382 - val_acc: 0.9011\n",
      "Epoch 162/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2237 - acc: 0.9042 - val_loss: 0.2406 - val_acc: 0.9009\n",
      "Epoch 163/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2244 - acc: 0.9041 - val_loss: 0.2399 - val_acc: 0.9026\n",
      "Epoch 164/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2239 - acc: 0.9046 - val_loss: 0.2374 - val_acc: 0.9034\n",
      "Epoch 165/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2240 - acc: 0.9040 - val_loss: 0.2423 - val_acc: 0.9036\n",
      "Epoch 166/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2245 - acc: 0.9047 - val_loss: 0.2406 - val_acc: 0.9015\n",
      "Epoch 167/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2242 - acc: 0.9038 - val_loss: 0.2363 - val_acc: 0.9017\n",
      "Epoch 168/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2246 - acc: 0.9049 - val_loss: 0.2376 - val_acc: 0.9032\n",
      "Epoch 169/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2240 - acc: 0.9051 - val_loss: 0.2414 - val_acc: 0.9003\n",
      "Epoch 170/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2245 - acc: 0.9049 - val_loss: 0.2425 - val_acc: 0.9032\n",
      "Epoch 171/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2237 - acc: 0.9043 - val_loss: 0.2373 - val_acc: 0.9026\n",
      "Epoch 172/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2247 - acc: 0.9037 - val_loss: 0.2379 - val_acc: 0.9019\n",
      "Epoch 173/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2235 - acc: 0.9051 - val_loss: 0.2380 - val_acc: 0.9021\n",
      "Epoch 174/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2242 - acc: 0.9044 - val_loss: 0.2397 - val_acc: 0.9017\n",
      "Epoch 175/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2242 - acc: 0.9037 - val_loss: 0.2477 - val_acc: 0.8999\n",
      "Epoch 176/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2246 - acc: 0.9041 - val_loss: 0.2407 - val_acc: 0.9030\n",
      "Epoch 177/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2247 - acc: 0.9027 - val_loss: 0.2388 - val_acc: 0.9015\n",
      "Epoch 178/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2234 - acc: 0.9054 - val_loss: 0.2382 - val_acc: 0.9030\n",
      "Epoch 179/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2235 - acc: 0.9043 - val_loss: 0.2366 - val_acc: 0.9032\n",
      "Epoch 180/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2236 - acc: 0.9044 - val_loss: 0.2408 - val_acc: 0.9028\n",
      "Epoch 181/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2238 - acc: 0.9043 - val_loss: 0.2379 - val_acc: 0.9032\n",
      "Epoch 182/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2238 - acc: 0.9049 - val_loss: 0.2369 - val_acc: 0.9036\n",
      "Epoch 183/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2241 - acc: 0.9046 - val_loss: 0.2376 - val_acc: 0.9011\n",
      "Epoch 184/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2242 - acc: 0.9050 - val_loss: 0.2401 - val_acc: 0.9021\n",
      "Epoch 185/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2236 - acc: 0.9044 - val_loss: 0.2365 - val_acc: 0.9023\n",
      "Epoch 186/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2237 - acc: 0.9039 - val_loss: 0.2366 - val_acc: 0.9005\n",
      "Epoch 187/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2240 - acc: 0.9041 - val_loss: 0.2374 - val_acc: 0.9019\n",
      "Epoch 188/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2233 - acc: 0.9028 - val_loss: 0.2375 - val_acc: 0.9017\n",
      "Epoch 189/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2240 - acc: 0.9036 - val_loss: 0.2384 - val_acc: 0.9019\n",
      "Epoch 190/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2237 - acc: 0.9039 - val_loss: 0.2371 - val_acc: 0.9034\n",
      "Epoch 191/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2241 - acc: 0.9030 - val_loss: 0.2366 - val_acc: 0.9032\n",
      "Epoch 192/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2237 - acc: 0.9034 - val_loss: 0.2422 - val_acc: 0.9011\n",
      "Epoch 193/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2241 - acc: 0.9041 - val_loss: 0.2380 - val_acc: 0.9034\n",
      "Epoch 194/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2242 - acc: 0.9050 - val_loss: 0.2408 - val_acc: 0.9023\n",
      "Epoch 195/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2232 - acc: 0.9043 - val_loss: 0.2367 - val_acc: 0.9009\n",
      "Epoch 196/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2237 - acc: 0.9045 - val_loss: 0.2364 - val_acc: 0.9015\n",
      "Epoch 197/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2240 - acc: 0.9041 - val_loss: 0.2371 - val_acc: 0.9023\n",
      "Epoch 198/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2232 - acc: 0.9050 - val_loss: 0.2425 - val_acc: 0.9040\n",
      "Epoch 199/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2239 - acc: 0.9042 - val_loss: 0.2376 - val_acc: 0.9017\n",
      "Epoch 200/500\n",
      "20253/20253 [==============================] - 1s 33us/step - loss: 0.2242 - acc: 0.9044 - val_loss: 0.2369 - val_acc: 0.9028\n",
      "Epoch 201/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2241 - acc: 0.9044 - val_loss: 0.2368 - val_acc: 0.9028\n",
      "Epoch 202/500\n",
      "20253/20253 [==============================] - 1s 34us/step - loss: 0.2232 - acc: 0.9047 - val_loss: 0.2397 - val_acc: 0.9023\n",
      "Epoch 203/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2234 - acc: 0.9050 - val_loss: 0.2400 - val_acc: 0.9026\n",
      "Epoch 204/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2232 - acc: 0.9033 - val_loss: 0.2460 - val_acc: 0.9032\n",
      "Epoch 205/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2237 - acc: 0.9051 - val_loss: 0.2379 - val_acc: 0.9024\n",
      "Epoch 206/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2235 - acc: 0.9037 - val_loss: 0.2400 - val_acc: 0.9026\n",
      "Epoch 207/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2236 - acc: 0.9038 - val_loss: 0.2378 - val_acc: 0.9042\n",
      "Epoch 208/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2235 - acc: 0.9053 - val_loss: 0.2367 - val_acc: 0.9009\n",
      "Epoch 209/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2232 - acc: 0.9044 - val_loss: 0.2375 - val_acc: 0.9023\n",
      "Epoch 210/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2228 - acc: 0.9044 - val_loss: 0.2378 - val_acc: 0.9024\n",
      "Epoch 211/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2236 - acc: 0.9050 - val_loss: 0.2364 - val_acc: 0.9042\n",
      "Epoch 212/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2231 - acc: 0.9040 - val_loss: 0.2364 - val_acc: 0.9021\n",
      "Epoch 213/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2238 - acc: 0.9048 - val_loss: 0.2376 - val_acc: 0.9013\n",
      "Epoch 214/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2239 - acc: 0.9037 - val_loss: 0.2383 - val_acc: 0.9044\n",
      "Epoch 215/500\n",
      "20253/20253 [==============================] - 1s 33us/step - loss: 0.2232 - acc: 0.9048 - val_loss: 0.2378 - val_acc: 0.9024\n",
      "Epoch 216/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2233 - acc: 0.9044 - val_loss: 0.2368 - val_acc: 0.9017\n",
      "Epoch 217/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2235 - acc: 0.9031 - val_loss: 0.2363 - val_acc: 0.9030\n",
      "Epoch 218/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2235 - acc: 0.9032 - val_loss: 0.2419 - val_acc: 0.9032\n",
      "Epoch 219/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2236 - acc: 0.9040 - val_loss: 0.2361 - val_acc: 0.9026\n",
      "Epoch 220/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2230 - acc: 0.9035 - val_loss: 0.2366 - val_acc: 0.9038\n",
      "Epoch 221/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2230 - acc: 0.9043 - val_loss: 0.2372 - val_acc: 0.9034\n",
      "Epoch 222/500\n",
      "20253/20253 [==============================] - ETA: 0s - loss: 0.2225 - acc: 0.905 - 1s 31us/step - loss: 0.2235 - acc: 0.9046 - val_loss: 0.2376 - val_acc: 0.9032\n",
      "Epoch 223/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2236 - acc: 0.9045 - val_loss: 0.2364 - val_acc: 0.9028\n",
      "Epoch 224/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2228 - acc: 0.9045 - val_loss: 0.2376 - val_acc: 0.9021\n",
      "Epoch 225/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2230 - acc: 0.9045 - val_loss: 0.2370 - val_acc: 0.9028\n",
      "Epoch 226/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2235 - acc: 0.9040 - val_loss: 0.2364 - val_acc: 0.9038\n",
      "Epoch 227/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2232 - acc: 0.9048 - val_loss: 0.2357 - val_acc: 0.9042\n",
      "Epoch 228/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2234 - acc: 0.9036 - val_loss: 0.2367 - val_acc: 0.9028\n",
      "Epoch 229/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2231 - acc: 0.9051 - val_loss: 0.2365 - val_acc: 0.9030\n",
      "Epoch 230/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2229 - acc: 0.9044 - val_loss: 0.2366 - val_acc: 0.9032\n",
      "Epoch 231/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2235 - acc: 0.9046 - val_loss: 0.2379 - val_acc: 0.9034\n",
      "Epoch 232/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2230 - acc: 0.9047 - val_loss: 0.2405 - val_acc: 0.9026\n",
      "Epoch 233/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2235 - acc: 0.9044 - val_loss: 0.2373 - val_acc: 0.9026\n",
      "Epoch 234/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2225 - acc: 0.9047 - val_loss: 0.2428 - val_acc: 0.9021\n",
      "Epoch 235/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2236 - acc: 0.9051 - val_loss: 0.2380 - val_acc: 0.9028\n",
      "Epoch 236/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2230 - acc: 0.9038 - val_loss: 0.2381 - val_acc: 0.9040\n",
      "Epoch 237/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2234 - acc: 0.9032 - val_loss: 0.2409 - val_acc: 0.9007\n",
      "Epoch 238/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2229 - acc: 0.9044 - val_loss: 0.2382 - val_acc: 0.9023\n",
      "Epoch 239/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2232 - acc: 0.9045 - val_loss: 0.2404 - val_acc: 0.9034\n",
      "Epoch 240/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2228 - acc: 0.9043 - val_loss: 0.2360 - val_acc: 0.9042\n",
      "Epoch 241/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2230 - acc: 0.9053 - val_loss: 0.2358 - val_acc: 0.9030\n",
      "Epoch 242/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2232 - acc: 0.9036 - val_loss: 0.2358 - val_acc: 0.9023\n",
      "Epoch 243/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2230 - acc: 0.9039 - val_loss: 0.2369 - val_acc: 0.9024\n",
      "Epoch 244/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2226 - acc: 0.9041 - val_loss: 0.2420 - val_acc: 0.9023\n",
      "Epoch 245/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2238 - acc: 0.9050 - val_loss: 0.2371 - val_acc: 0.9011\n",
      "Epoch 246/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2224 - acc: 0.9044 - val_loss: 0.2369 - val_acc: 0.9034\n",
      "Epoch 247/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2225 - acc: 0.9038 - val_loss: 0.2362 - val_acc: 0.9030\n",
      "Epoch 248/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2230 - acc: 0.9035 - val_loss: 0.2364 - val_acc: 0.9011\n",
      "Epoch 249/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2226 - acc: 0.9046 - val_loss: 0.2366 - val_acc: 0.9030\n",
      "Epoch 250/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2228 - acc: 0.9035 - val_loss: 0.2358 - val_acc: 0.9040\n",
      "Epoch 251/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2223 - acc: 0.9044 - val_loss: 0.2351 - val_acc: 0.9036\n",
      "Epoch 252/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2228 - acc: 0.9047 - val_loss: 0.2354 - val_acc: 0.9038\n",
      "Epoch 253/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2230 - acc: 0.9054 - val_loss: 0.2378 - val_acc: 0.9030\n",
      "Epoch 254/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2225 - acc: 0.9046 - val_loss: 0.2395 - val_acc: 0.9003\n",
      "Epoch 255/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2226 - acc: 0.9046 - val_loss: 0.2358 - val_acc: 0.9015\n",
      "Epoch 256/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2229 - acc: 0.9049 - val_loss: 0.2417 - val_acc: 0.9040\n",
      "Epoch 257/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2222 - acc: 0.9048 - val_loss: 0.2357 - val_acc: 0.9024\n",
      "Epoch 258/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2224 - acc: 0.9043 - val_loss: 0.2347 - val_acc: 0.9040\n",
      "Epoch 259/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2230 - acc: 0.9041 - val_loss: 0.2368 - val_acc: 0.9007\n",
      "Epoch 260/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2234 - acc: 0.9041 - val_loss: 0.2351 - val_acc: 0.9042\n",
      "Epoch 261/500\n",
      "20253/20253 [==============================] - 1s 29us/step - loss: 0.2229 - acc: 0.9029 - val_loss: 0.2347 - val_acc: 0.9024\n",
      "Epoch 262/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2224 - acc: 0.9046 - val_loss: 0.2365 - val_acc: 0.9024\n",
      "Epoch 263/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2228 - acc: 0.9044 - val_loss: 0.2365 - val_acc: 0.9040\n",
      "Epoch 264/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2231 - acc: 0.9040 - val_loss: 0.2353 - val_acc: 0.9030\n",
      "Epoch 265/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2227 - acc: 0.9040 - val_loss: 0.2388 - val_acc: 0.9038\n",
      "Epoch 266/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9044 - val_loss: 0.2383 - val_acc: 0.9023\n",
      "Epoch 267/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2229 - acc: 0.9042 - val_loss: 0.2358 - val_acc: 0.9021\n",
      "Epoch 268/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2231 - acc: 0.9038 - val_loss: 0.2355 - val_acc: 0.9032\n",
      "Epoch 269/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2226 - acc: 0.9040 - val_loss: 0.2367 - val_acc: 0.9024\n",
      "Epoch 270/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2226 - acc: 0.9045 - val_loss: 0.2353 - val_acc: 0.9044\n",
      "Epoch 271/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2219 - acc: 0.9045 - val_loss: 0.2376 - val_acc: 0.9024\n",
      "Epoch 272/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2225 - acc: 0.9046 - val_loss: 0.2363 - val_acc: 0.9032\n",
      "Epoch 273/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2228 - acc: 0.9032 - val_loss: 0.2347 - val_acc: 0.9026\n",
      "Epoch 274/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2226 - acc: 0.9042 - val_loss: 0.2354 - val_acc: 0.9021\n",
      "Epoch 275/500\n",
      "20253/20253 [==============================] - 1s 33us/step - loss: 0.2234 - acc: 0.9041 - val_loss: 0.2343 - val_acc: 0.9026\n",
      "Epoch 276/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2223 - acc: 0.9044 - val_loss: 0.2383 - val_acc: 0.9034\n",
      "Epoch 277/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2225 - acc: 0.9035 - val_loss: 0.2362 - val_acc: 0.9048\n",
      "Epoch 278/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2222 - acc: 0.9035 - val_loss: 0.2359 - val_acc: 0.9032\n",
      "Epoch 279/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2225 - acc: 0.9042 - val_loss: 0.2363 - val_acc: 0.9036\n",
      "Epoch 280/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2226 - acc: 0.9047 - val_loss: 0.2356 - val_acc: 0.9026\n",
      "Epoch 281/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2223 - acc: 0.9049 - val_loss: 0.2370 - val_acc: 0.9021\n",
      "Epoch 282/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2230 - acc: 0.9039 - val_loss: 0.2376 - val_acc: 0.9019\n",
      "Epoch 283/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2223 - acc: 0.9053 - val_loss: 0.2375 - val_acc: 0.9028\n",
      "Epoch 284/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2220 - acc: 0.9047 - val_loss: 0.2361 - val_acc: 0.9034\n",
      "Epoch 285/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2224 - acc: 0.9039 - val_loss: 0.2364 - val_acc: 0.9015\n",
      "Epoch 286/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2228 - acc: 0.9041 - val_loss: 0.2356 - val_acc: 0.9034\n",
      "Epoch 287/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2222 - acc: 0.9040 - val_loss: 0.2354 - val_acc: 0.9050\n",
      "Epoch 288/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2220 - acc: 0.9051 - val_loss: 0.2357 - val_acc: 0.9034\n",
      "Epoch 289/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2224 - acc: 0.9039 - val_loss: 0.2367 - val_acc: 0.9028\n",
      "Epoch 290/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2223 - acc: 0.9040 - val_loss: 0.2419 - val_acc: 0.9023\n",
      "Epoch 291/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2220 - acc: 0.9046 - val_loss: 0.2365 - val_acc: 0.9034\n",
      "Epoch 292/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2233 - acc: 0.9039 - val_loss: 0.2363 - val_acc: 0.9030\n",
      "Epoch 293/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9051 - val_loss: 0.2367 - val_acc: 0.9026\n",
      "Epoch 294/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2225 - acc: 0.9039 - val_loss: 0.2356 - val_acc: 0.9024\n",
      "Epoch 295/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2224 - acc: 0.9034 - val_loss: 0.2432 - val_acc: 0.9021\n",
      "Epoch 296/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2222 - acc: 0.9039 - val_loss: 0.2356 - val_acc: 0.9030\n",
      "Epoch 297/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2226 - acc: 0.9036 - val_loss: 0.2360 - val_acc: 0.9036\n",
      "Epoch 298/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2221 - acc: 0.9047 - val_loss: 0.2350 - val_acc: 0.9023\n",
      "Epoch 299/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2217 - acc: 0.9041 - val_loss: 0.2370 - val_acc: 0.9034\n",
      "Epoch 300/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2228 - acc: 0.9045 - val_loss: 0.2353 - val_acc: 0.9007\n",
      "Epoch 301/500\n",
      "20253/20253 [==============================] - 1s 29us/step - loss: 0.2225 - acc: 0.9045 - val_loss: 0.2351 - val_acc: 0.9036\n",
      "Epoch 302/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9050 - val_loss: 0.2372 - val_acc: 0.9024\n",
      "Epoch 303/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2227 - acc: 0.9045 - val_loss: 0.2361 - val_acc: 0.9019\n",
      "Epoch 304/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9045 - val_loss: 0.2345 - val_acc: 0.9034\n",
      "Epoch 305/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9046 - val_loss: 0.2377 - val_acc: 0.9026\n",
      "Epoch 306/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2222 - acc: 0.9034 - val_loss: 0.2373 - val_acc: 0.9017\n",
      "Epoch 307/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2225 - acc: 0.9043 - val_loss: 0.2369 - val_acc: 0.9026\n",
      "Epoch 308/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2221 - acc: 0.9043 - val_loss: 0.2355 - val_acc: 0.9036\n",
      "Epoch 309/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2226 - acc: 0.9044 - val_loss: 0.2412 - val_acc: 0.9050\n",
      "Epoch 310/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2229 - acc: 0.9054 - val_loss: 0.2373 - val_acc: 0.9019\n",
      "Epoch 311/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2220 - acc: 0.9052 - val_loss: 0.2347 - val_acc: 0.9017\n",
      "Epoch 312/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2227 - acc: 0.9044 - val_loss: 0.2365 - val_acc: 0.9034\n",
      "Epoch 313/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9039 - val_loss: 0.2382 - val_acc: 0.9050\n",
      "Epoch 314/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2220 - acc: 0.9042 - val_loss: 0.2403 - val_acc: 0.9026\n",
      "Epoch 315/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2219 - acc: 0.9056 - val_loss: 0.2371 - val_acc: 0.9036\n",
      "Epoch 316/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9026 - val_loss: 0.2361 - val_acc: 0.9040\n",
      "Epoch 317/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2226 - acc: 0.9046 - val_loss: 0.2361 - val_acc: 0.9036\n",
      "Epoch 318/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2220 - acc: 0.9045 - val_loss: 0.2352 - val_acc: 0.8999\n",
      "Epoch 319/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2222 - acc: 0.9038 - val_loss: 0.2365 - val_acc: 0.9019\n",
      "Epoch 320/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2223 - acc: 0.9043 - val_loss: 0.2361 - val_acc: 0.9032\n",
      "Epoch 321/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9042 - val_loss: 0.2374 - val_acc: 0.9001\n",
      "Epoch 322/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9044 - val_loss: 0.2345 - val_acc: 0.9026\n",
      "Epoch 323/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2222 - acc: 0.9040 - val_loss: 0.2356 - val_acc: 0.9023\n",
      "Epoch 324/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2230 - acc: 0.9039 - val_loss: 0.2356 - val_acc: 0.9024\n",
      "Epoch 325/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2220 - acc: 0.9035 - val_loss: 0.2340 - val_acc: 0.9021\n",
      "Epoch 326/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2222 - acc: 0.9038 - val_loss: 0.2371 - val_acc: 0.9040\n",
      "Epoch 327/500\n",
      "20253/20253 [==============================] - 1s 29us/step - loss: 0.2215 - acc: 0.9033 - val_loss: 0.2354 - val_acc: 0.9021\n",
      "Epoch 328/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2214 - acc: 0.9040 - val_loss: 0.2433 - val_acc: 0.9023\n",
      "Epoch 329/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2228 - acc: 0.9036 - val_loss: 0.2365 - val_acc: 0.9023\n",
      "Epoch 330/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9039 - val_loss: 0.2377 - val_acc: 0.9034\n",
      "Epoch 331/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2223 - acc: 0.9039 - val_loss: 0.2348 - val_acc: 0.9026\n",
      "Epoch 332/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2216 - acc: 0.9049 - val_loss: 0.2352 - val_acc: 0.9028\n",
      "Epoch 333/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9037 - val_loss: 0.2357 - val_acc: 0.9036\n",
      "Epoch 334/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9040 - val_loss: 0.2362 - val_acc: 0.9034\n",
      "Epoch 335/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2221 - acc: 0.9035 - val_loss: 0.2356 - val_acc: 0.9021\n",
      "Epoch 336/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2217 - acc: 0.9048 - val_loss: 0.2373 - val_acc: 0.9042\n",
      "Epoch 337/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2225 - acc: 0.9034 - val_loss: 0.2354 - val_acc: 0.9034\n",
      "Epoch 338/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2215 - acc: 0.9043 - val_loss: 0.2405 - val_acc: 0.9005\n",
      "Epoch 339/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9038 - val_loss: 0.2398 - val_acc: 0.9015\n",
      "Epoch 340/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2219 - acc: 0.9040 - val_loss: 0.2359 - val_acc: 0.9026\n",
      "Epoch 341/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9042 - val_loss: 0.2352 - val_acc: 0.9023\n",
      "Epoch 342/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9047 - val_loss: 0.2401 - val_acc: 0.9015\n",
      "Epoch 343/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2221 - acc: 0.9045 - val_loss: 0.2437 - val_acc: 0.9034\n",
      "Epoch 344/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2221 - acc: 0.9033 - val_loss: 0.2358 - val_acc: 0.9017\n",
      "Epoch 345/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2223 - acc: 0.9037 - val_loss: 0.2374 - val_acc: 0.9030\n",
      "Epoch 346/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9043 - val_loss: 0.2353 - val_acc: 0.9015\n",
      "Epoch 347/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2223 - acc: 0.9041 - val_loss: 0.2359 - val_acc: 0.9017\n",
      "Epoch 348/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9049 - val_loss: 0.2362 - val_acc: 0.9026\n",
      "Epoch 349/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2217 - acc: 0.9035 - val_loss: 0.2367 - val_acc: 0.9013\n",
      "Epoch 350/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2216 - acc: 0.9046 - val_loss: 0.2371 - val_acc: 0.9032\n",
      "Epoch 351/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2216 - acc: 0.9039 - val_loss: 0.2368 - val_acc: 0.9034\n",
      "Epoch 352/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9041 - val_loss: 0.2372 - val_acc: 0.9009\n",
      "Epoch 353/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9051 - val_loss: 0.2379 - val_acc: 0.9026\n",
      "Epoch 354/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2220 - acc: 0.9040 - val_loss: 0.2353 - val_acc: 0.9032\n",
      "Epoch 355/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2222 - acc: 0.9034 - val_loss: 0.2395 - val_acc: 0.9032\n",
      "Epoch 356/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9038 - val_loss: 0.2390 - val_acc: 0.9030\n",
      "Epoch 357/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2213 - acc: 0.9047 - val_loss: 0.2361 - val_acc: 0.9011\n",
      "Epoch 358/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2223 - acc: 0.9048 - val_loss: 0.2356 - val_acc: 0.9015\n",
      "Epoch 359/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2212 - acc: 0.9050 - val_loss: 0.2363 - val_acc: 0.9032\n",
      "Epoch 360/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2213 - acc: 0.9047 - val_loss: 0.2364 - val_acc: 0.9023\n",
      "Epoch 361/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9037 - val_loss: 0.2355 - val_acc: 0.9019\n",
      "Epoch 362/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2229 - acc: 0.9041 - val_loss: 0.2386 - val_acc: 0.9036\n",
      "Epoch 363/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2226 - acc: 0.9043 - val_loss: 0.2351 - val_acc: 0.9030\n",
      "Epoch 364/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9045 - val_loss: 0.2357 - val_acc: 0.9013\n",
      "Epoch 365/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2214 - acc: 0.9041 - val_loss: 0.2346 - val_acc: 0.9034\n",
      "Epoch 366/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9032 - val_loss: 0.2358 - val_acc: 0.9023\n",
      "Epoch 367/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2217 - acc: 0.9053 - val_loss: 0.2361 - val_acc: 0.9042\n",
      "Epoch 368/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2221 - acc: 0.9047 - val_loss: 0.2371 - val_acc: 0.9021\n",
      "Epoch 369/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2211 - acc: 0.9046 - val_loss: 0.2402 - val_acc: 0.8999\n",
      "Epoch 370/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2226 - acc: 0.9031 - val_loss: 0.2370 - val_acc: 0.9023\n",
      "Epoch 371/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2214 - acc: 0.9038 - val_loss: 0.2347 - val_acc: 0.9036\n",
      "Epoch 372/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2217 - acc: 0.9041 - val_loss: 0.2356 - val_acc: 0.9013\n",
      "Epoch 373/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2220 - acc: 0.9036 - val_loss: 0.2378 - val_acc: 0.9026\n",
      "Epoch 374/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2213 - acc: 0.9048 - val_loss: 0.2356 - val_acc: 0.9015\n",
      "Epoch 375/500\n",
      "20253/20253 [==============================] - ETA: 0s - loss: 0.2208 - acc: 0.905 - 1s 31us/step - loss: 0.2217 - acc: 0.9046 - val_loss: 0.2360 - val_acc: 0.9011\n",
      "Epoch 376/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2222 - acc: 0.9049 - val_loss: 0.2364 - val_acc: 0.9034\n",
      "Epoch 377/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2216 - acc: 0.9036 - val_loss: 0.2355 - val_acc: 0.9024\n",
      "Epoch 378/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2208 - acc: 0.9037 - val_loss: 0.2449 - val_acc: 0.9021\n",
      "Epoch 379/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9037 - val_loss: 0.2350 - val_acc: 0.9036\n",
      "Epoch 380/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2210 - acc: 0.9048 - val_loss: 0.2376 - val_acc: 0.9030\n",
      "Epoch 381/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9042 - val_loss: 0.2356 - val_acc: 0.9023\n",
      "Epoch 382/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9039 - val_loss: 0.2374 - val_acc: 0.9032\n",
      "Epoch 383/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2214 - acc: 0.9039 - val_loss: 0.2371 - val_acc: 0.9032\n",
      "Epoch 384/500\n",
      "20253/20253 [==============================] - 1s 29us/step - loss: 0.2215 - acc: 0.9041 - val_loss: 0.2355 - val_acc: 0.9032\n",
      "Epoch 385/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9051 - val_loss: 0.2361 - val_acc: 0.9038\n",
      "Epoch 386/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2213 - acc: 0.9046 - val_loss: 0.2356 - val_acc: 0.9028\n",
      "Epoch 387/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9049 - val_loss: 0.2450 - val_acc: 0.9007\n",
      "Epoch 388/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2219 - acc: 0.9043 - val_loss: 0.2412 - val_acc: 0.9030\n",
      "Epoch 389/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9049 - val_loss: 0.2369 - val_acc: 0.9028\n",
      "Epoch 390/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9036 - val_loss: 0.2361 - val_acc: 0.9026\n",
      "Epoch 391/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2215 - acc: 0.9048 - val_loss: 0.2347 - val_acc: 0.9019\n",
      "Epoch 392/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9043 - val_loss: 0.2357 - val_acc: 0.9019\n",
      "Epoch 393/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2216 - acc: 0.9048 - val_loss: 0.2367 - val_acc: 0.9021\n",
      "Epoch 394/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2220 - acc: 0.9035 - val_loss: 0.2353 - val_acc: 0.9015\n",
      "Epoch 395/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2214 - acc: 0.9054 - val_loss: 0.2359 - val_acc: 0.9028\n",
      "Epoch 396/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2214 - acc: 0.9044 - val_loss: 0.2359 - val_acc: 0.9026\n",
      "Epoch 397/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2216 - acc: 0.9039 - val_loss: 0.2367 - val_acc: 0.9023\n",
      "Epoch 398/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9042 - val_loss: 0.2352 - val_acc: 0.9021\n",
      "Epoch 399/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2209 - acc: 0.9052 - val_loss: 0.2356 - val_acc: 0.9017\n",
      "Epoch 400/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2216 - acc: 0.9041 - val_loss: 0.2358 - val_acc: 0.9015\n",
      "Epoch 401/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2216 - acc: 0.9049 - val_loss: 0.2358 - val_acc: 0.9023\n",
      "Epoch 402/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2215 - acc: 0.9048 - val_loss: 0.2388 - val_acc: 0.9028\n",
      "Epoch 403/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2210 - acc: 0.9043 - val_loss: 0.2418 - val_acc: 0.9026\n",
      "Epoch 404/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2217 - acc: 0.9040 - val_loss: 0.2359 - val_acc: 0.9019\n",
      "Epoch 405/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2214 - acc: 0.9049 - val_loss: 0.2362 - val_acc: 0.9023\n",
      "Epoch 406/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2213 - acc: 0.9051 - val_loss: 0.2368 - val_acc: 0.8991\n",
      "Epoch 407/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2208 - acc: 0.9047 - val_loss: 0.2366 - val_acc: 0.9017\n",
      "Epoch 408/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2211 - acc: 0.9041 - val_loss: 0.2356 - val_acc: 0.9013\n",
      "Epoch 409/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2222 - acc: 0.9046 - val_loss: 0.2362 - val_acc: 0.9011\n",
      "Epoch 410/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9042 - val_loss: 0.2354 - val_acc: 0.9015\n",
      "Epoch 411/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2216 - acc: 0.9035 - val_loss: 0.2378 - val_acc: 0.9024\n",
      "Epoch 412/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2208 - acc: 0.9034 - val_loss: 0.2364 - val_acc: 0.9007\n",
      "Epoch 413/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2214 - acc: 0.9042 - val_loss: 0.2363 - val_acc: 0.9015\n",
      "Epoch 414/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9045 - val_loss: 0.2379 - val_acc: 0.9032\n",
      "Epoch 415/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2216 - acc: 0.9043 - val_loss: 0.2365 - val_acc: 0.9005\n",
      "Epoch 416/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2221 - acc: 0.9048 - val_loss: 0.2360 - val_acc: 0.9028\n",
      "Epoch 417/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9040 - val_loss: 0.2364 - val_acc: 0.9009\n",
      "Epoch 418/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2209 - acc: 0.9048 - val_loss: 0.2438 - val_acc: 0.9007\n",
      "Epoch 419/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2221 - acc: 0.9051 - val_loss: 0.2388 - val_acc: 0.9009\n",
      "Epoch 420/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2213 - acc: 0.9042 - val_loss: 0.2381 - val_acc: 0.9026\n",
      "Epoch 421/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2215 - acc: 0.9051 - val_loss: 0.2358 - val_acc: 0.9026\n",
      "Epoch 422/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2205 - acc: 0.9042 - val_loss: 0.2375 - val_acc: 0.9023\n",
      "Epoch 423/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2209 - acc: 0.9051 - val_loss: 0.2358 - val_acc: 0.9028\n",
      "Epoch 424/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9048 - val_loss: 0.2356 - val_acc: 0.9011\n",
      "Epoch 425/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2211 - acc: 0.9045 - val_loss: 0.2379 - val_acc: 0.9030\n",
      "Epoch 426/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2215 - acc: 0.9046 - val_loss: 0.2354 - val_acc: 0.9013\n",
      "Epoch 427/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2213 - acc: 0.9052 - val_loss: 0.2362 - val_acc: 0.9015\n",
      "Epoch 428/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2214 - acc: 0.9050 - val_loss: 0.2377 - val_acc: 0.9032\n",
      "Epoch 429/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2208 - acc: 0.9041 - val_loss: 0.2362 - val_acc: 0.9021\n",
      "Epoch 430/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2209 - acc: 0.9054 - val_loss: 0.2400 - val_acc: 0.9028\n",
      "Epoch 431/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2211 - acc: 0.9055 - val_loss: 0.2355 - val_acc: 0.9038\n",
      "Epoch 432/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2208 - acc: 0.9047 - val_loss: 0.2355 - val_acc: 0.9011\n",
      "Epoch 433/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2212 - acc: 0.9047 - val_loss: 0.2358 - val_acc: 0.9023\n",
      "Epoch 434/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2216 - acc: 0.9062 - val_loss: 0.2377 - val_acc: 0.9032\n",
      "Epoch 435/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9038 - val_loss: 0.2397 - val_acc: 0.9019\n",
      "Epoch 436/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2210 - acc: 0.9047 - val_loss: 0.2388 - val_acc: 0.9011\n",
      "Epoch 437/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2215 - acc: 0.9041 - val_loss: 0.2365 - val_acc: 0.9032\n",
      "Epoch 438/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2211 - acc: 0.9045 - val_loss: 0.2371 - val_acc: 0.9024\n",
      "Epoch 439/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2213 - acc: 0.9046 - val_loss: 0.2362 - val_acc: 0.9019\n",
      "Epoch 440/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9035 - val_loss: 0.2374 - val_acc: 0.9023\n",
      "Epoch 441/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2213 - acc: 0.9048 - val_loss: 0.2350 - val_acc: 0.9034\n",
      "Epoch 442/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2211 - acc: 0.9050 - val_loss: 0.2382 - val_acc: 0.9026\n",
      "Epoch 443/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9050 - val_loss: 0.2381 - val_acc: 0.9009\n",
      "Epoch 444/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2208 - acc: 0.9053 - val_loss: 0.2356 - val_acc: 0.9019\n",
      "Epoch 445/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9055 - val_loss: 0.2362 - val_acc: 0.9019\n",
      "Epoch 446/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9048 - val_loss: 0.2358 - val_acc: 0.9030\n",
      "Epoch 447/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9039 - val_loss: 0.2366 - val_acc: 0.9007\n",
      "Epoch 448/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2210 - acc: 0.9037 - val_loss: 0.2392 - val_acc: 0.9019\n",
      "Epoch 449/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2210 - acc: 0.9037 - val_loss: 0.2380 - val_acc: 0.9024\n",
      "Epoch 450/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2210 - acc: 0.9050 - val_loss: 0.2378 - val_acc: 0.9021\n",
      "Epoch 451/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9050 - val_loss: 0.2369 - val_acc: 0.9007\n",
      "Epoch 452/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2218 - acc: 0.9046 - val_loss: 0.2357 - val_acc: 0.9003\n",
      "Epoch 453/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9045 - val_loss: 0.2364 - val_acc: 0.9019\n",
      "Epoch 454/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2218 - acc: 0.9048 - val_loss: 0.2363 - val_acc: 0.9007\n",
      "Epoch 455/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9043 - val_loss: 0.2357 - val_acc: 0.9021\n",
      "Epoch 456/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2215 - acc: 0.9049 - val_loss: 0.2373 - val_acc: 0.9024\n",
      "Epoch 457/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2207 - acc: 0.9045 - val_loss: 0.2366 - val_acc: 0.8995\n",
      "Epoch 458/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2210 - acc: 0.9043 - val_loss: 0.2387 - val_acc: 0.9040\n",
      "Epoch 459/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2213 - acc: 0.9056 - val_loss: 0.2359 - val_acc: 0.9011\n",
      "Epoch 460/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2209 - acc: 0.9054 - val_loss: 0.2368 - val_acc: 0.9021\n",
      "Epoch 461/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9040 - val_loss: 0.2397 - val_acc: 0.9013\n",
      "Epoch 462/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9053 - val_loss: 0.2371 - val_acc: 0.9030\n",
      "Epoch 463/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2222 - acc: 0.9049 - val_loss: 0.2364 - val_acc: 0.9007\n",
      "Epoch 464/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2211 - acc: 0.9042 - val_loss: 0.2371 - val_acc: 0.9028\n",
      "Epoch 465/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9042 - val_loss: 0.2420 - val_acc: 0.9007\n",
      "Epoch 466/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2214 - acc: 0.9049 - val_loss: 0.2381 - val_acc: 0.9030\n",
      "Epoch 467/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2206 - acc: 0.9047 - val_loss: 0.2357 - val_acc: 0.9038\n",
      "Epoch 468/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2209 - acc: 0.9053 - val_loss: 0.2374 - val_acc: 0.9026\n",
      "Epoch 469/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2211 - acc: 0.9051 - val_loss: 0.2386 - val_acc: 0.9003\n",
      "Epoch 470/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9044 - val_loss: 0.2436 - val_acc: 0.9024\n",
      "Epoch 471/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2210 - acc: 0.9034 - val_loss: 0.2384 - val_acc: 0.9026\n",
      "Epoch 472/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9039 - val_loss: 0.2373 - val_acc: 0.9032\n",
      "Epoch 473/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2215 - acc: 0.9050 - val_loss: 0.2352 - val_acc: 0.9021\n",
      "Epoch 474/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9043 - val_loss: 0.2452 - val_acc: 0.9017\n",
      "Epoch 475/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2207 - acc: 0.9043 - val_loss: 0.2438 - val_acc: 0.9017\n",
      "Epoch 476/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2211 - acc: 0.9045 - val_loss: 0.2354 - val_acc: 0.9015\n",
      "Epoch 477/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2211 - acc: 0.9054 - val_loss: 0.2371 - val_acc: 0.9023\n",
      "Epoch 478/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2217 - acc: 0.9037 - val_loss: 0.2390 - val_acc: 0.9019\n",
      "Epoch 479/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2206 - acc: 0.9052 - val_loss: 0.2432 - val_acc: 0.9011\n",
      "Epoch 480/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9055 - val_loss: 0.2354 - val_acc: 0.9009\n",
      "Epoch 481/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2208 - acc: 0.9055 - val_loss: 0.2358 - val_acc: 0.9021\n",
      "Epoch 482/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2210 - acc: 0.9046 - val_loss: 0.2380 - val_acc: 0.9003\n",
      "Epoch 483/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2219 - acc: 0.9048 - val_loss: 0.2379 - val_acc: 0.9021\n",
      "Epoch 484/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2213 - acc: 0.9045 - val_loss: 0.2362 - val_acc: 0.9034\n",
      "Epoch 485/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2207 - acc: 0.9047 - val_loss: 0.2365 - val_acc: 0.9009\n",
      "Epoch 486/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2212 - acc: 0.9055 - val_loss: 0.2358 - val_acc: 0.9017\n",
      "Epoch 487/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2203 - acc: 0.9053 - val_loss: 0.2356 - val_acc: 0.9021\n",
      "Epoch 488/500\n",
      "20253/20253 [==============================] - 1s 33us/step - loss: 0.2205 - acc: 0.9042 - val_loss: 0.2382 - val_acc: 0.9028\n",
      "Epoch 489/500\n",
      "20253/20253 [==============================] - 1s 29us/step - loss: 0.2205 - acc: 0.9052 - val_loss: 0.2371 - val_acc: 0.9011\n",
      "Epoch 490/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2209 - acc: 0.9056 - val_loss: 0.2365 - val_acc: 0.9026\n",
      "Epoch 491/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2207 - acc: 0.9047 - val_loss: 0.2376 - val_acc: 0.9021\n",
      "Epoch 492/500\n",
      "20253/20253 [==============================] - 1s 32us/step - loss: 0.2209 - acc: 0.9048 - val_loss: 0.2354 - val_acc: 0.9019\n",
      "Epoch 493/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2206 - acc: 0.9051 - val_loss: 0.2362 - val_acc: 0.9021\n",
      "Epoch 494/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2210 - acc: 0.9044 - val_loss: 0.2366 - val_acc: 0.9013\n",
      "Epoch 495/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2204 - acc: 0.9044 - val_loss: 0.2365 - val_acc: 0.8985\n",
      "Epoch 496/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2213 - acc: 0.9051 - val_loss: 0.2354 - val_acc: 0.9024\n",
      "Epoch 497/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2210 - acc: 0.9050 - val_loss: 0.2358 - val_acc: 0.9026\n",
      "Epoch 498/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2210 - acc: 0.9051 - val_loss: 0.2359 - val_acc: 0.9001\n",
      "Epoch 499/500\n",
      "20253/20253 [==============================] - 1s 30us/step - loss: 0.2206 - acc: 0.9053 - val_loss: 0.2374 - val_acc: 0.9017\n",
      "Epoch 500/500\n",
      "20253/20253 [==============================] - 1s 31us/step - loss: 0.2209 - acc: 0.9061 - val_loss: 0.2393 - val_acc: 0.9023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f8bfb70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn_model.fit(x_train, y_train, batch_size=50, epochs=500, verbose=1, validation_data=(x_val, y_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fcn_pred = fcn_model.predict_proba(test_set)\n",
    "result_fcn_pred = pd.DataFrame(dataset_test.iloc[:,:].ID, columns=['ID'])\n",
    "result_fcn_pred['pred'] = test_fcn_pred\n",
    "result_fcn_pred.to_csv('result_fcn_20190329_standard.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25318</td>\n",
       "      <td>0.035096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25319</td>\n",
       "      <td>0.009581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25320</td>\n",
       "      <td>0.005733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25321</td>\n",
       "      <td>0.565842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25322</td>\n",
       "      <td>0.016377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID      pred\n",
       "0  25318  0.035096\n",
       "1  25319  0.009581\n",
       "2  25320  0.005733\n",
       "3  25321  0.565842\n",
       "4  25322  0.016377"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_fcn_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5076</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2974</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  age  job  marital  education  default  balance  housing  loan  contact  \\\n",
       "0   1   43    4        1          2        0      291        1     0        2   \n",
       "1   2   42    9        0          0        0     5076        1     0        0   \n",
       "2   3   47    0        1          1        0      104        1     1        0   \n",
       "3   4   28    4        2          1        0     -994        1     1        0   \n",
       "4   5   42    9        0          1        0     2974        1     0        2   \n",
       "\n",
       "   day  month  duration  campaign  pdays  previous  poutcome  y  \n",
       "0    9      8       150         2     -1         0         3  0  \n",
       "1    7      0        99         1    251         2         1  0  \n",
       "2   14      5        77         2     -1         0         3  0  \n",
       "3   18      5       174         2     -1         0         3  0  \n",
       "4   21      8       187         5     -1         0         3  0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "age_data = dict(Counter(dataset_train.iloc[:,1]))\n",
    "plt.scatter(age_data.keys(), age_data.values())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=25317, output_dim=2, input_length=16))\n",
    "    model.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = sequence.pad_sequences(x_train, maxlen=500)\n",
    "val_x = sequence.pad_sequences(x_val, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 16, 2)             50634     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 16)            112       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 32)             3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 56,199\n",
      "Trainable params: 56,199\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20253 samples, validate on 5064 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[0,7] = -1 is not in [0, 25317)\n\t [[Node: embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast, training/Adam/gradients/embedding_1/embedding_lookup_grad/concat/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7045fd0d3a99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mE:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\Software\\envs\\py3.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[0,7] = -1 is not in [0, 25317)\n\t [[Node: embedding_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1/Cast, training/Adam/gradients/embedding_1/embedding_lookup_grad/concat/axis)]]"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=100, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_np = np.array(x_train)\n",
    "train_y_np = np.array(y_train)\n",
    "val_x_np = np.array(val_x)\n",
    "val_y_np = np.array(val_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
